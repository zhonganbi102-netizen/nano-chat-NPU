#!/bin/bash

echo "ğŸ§ª å°æ•°æ®å¤šGPUæµ‹è¯•..."

# ç¯å¢ƒå˜é‡è®¾ç½®
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500

# HCCLè®¾ç½®
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1

# å†…å­˜è®¾ç½®
export PYTORCH_NPU_ALLOC_CONF="max_split_size_mb:128"

# æ¸…ç†
pkill -f "python.*base_train.py" 2>/dev/null || true
pkill -f "torchrun" 2>/dev/null || true
sleep 3

echo "é…ç½®ä¿¡æ¯ï¼š"
echo "  ğŸ¯ 4ä¸ªNPU (0,1,2,3)"
echo "  ğŸ“Š è¶…å°æ¨¡å‹ï¼šdepth=3, æ¯å±‚å¾ˆå°‘å‚æ•°"
echo "  ğŸ’¾ æœ€å°batchï¼šdevice_batch_size=1"
echo "  ğŸ“ çŸ­åºåˆ—ï¼šmax_seq_len=256"
echo "  ğŸ”¢ å°‘æ­¥æ•°ï¼šä»…5æ­¥è®­ç»ƒ"
echo "  â­ï¸  è·³è¿‡æ‰€æœ‰è¯„ä¼°ï¼Œä¸“æ³¨è®­ç»ƒå¾ªç¯"
echo ""

echo "å¯åŠ¨å°æ•°æ®4NPUæµ‹è¯•..."

torchrun --standalone --nproc_per_node=4 -- scripts/base_train.py \
    --depth=3 \
    --device_batch_size=1 \
    --total_batch_size=4096 \
    --max_seq_len=256 \
    --num_iterations=5 \
    --eval_every=999999 \
    --core_metric_every=999999 \
    --sample_every=999999 \
    --run="small_data_4npu_$(date +%Y%m%d_%H%M%S)"

if [ $? -eq 0 ]; then
    echo ""
    echo "ğŸ‰ å°æ•°æ®4NPUæµ‹è¯•æˆåŠŸï¼"
    echo "ç°åœ¨å¯ä»¥é€æ­¥å¢åŠ ï¼š"
    echo "  1. å¢åŠ depthåˆ°6"
    echo "  2. å¢åŠ batch_sizeåˆ°2"  
    echo "  3. å¢åŠ è®­ç»ƒæ­¥æ•°åˆ°20"
else
    echo ""
    echo "âŒ å°æ•°æ®4NPUæµ‹è¯•å¤±è´¥"
    echo "å¯èƒ½éœ€è¦è¿›ä¸€æ­¥å‡å°é…ç½®"
fi

echo ""
echo "å°æ•°æ®æµ‹è¯•å®Œæˆ: $(date)"