#!/bin/bash

# ç»ˆæž4NPUè®­ç»ƒè§£å†³æ–¹æ¡ˆ - å¼ºåŠ›çŽ¯å¢ƒä¼ é€’
# ä½¿ç”¨wrapperè„šæœ¬ç¡®ä¿torchrunå­è¿›ç¨‹æ­£ç¡®ç»§æ‰¿çŽ¯å¢ƒ

set -e

echo "ðŸ’ª ç»ˆæž4NPU FineWebè®­ç»ƒ - å¼ºåŠ›çŽ¯å¢ƒè§£å†³æ–¹æ¡ˆ ðŸ’ª"

# 1. å¼ºåŠ›æ¸…ç†
echo "1. å¼ºåŠ›æ¸…ç†NPUçŽ¯å¢ƒ..."
./emergency_npu_cleanup.sh
sleep 20

# 2. åˆ›å»ºçŽ¯å¢ƒwrapperè„šæœ¬
echo "2. åˆ›å»ºçŽ¯å¢ƒwrapperè„šæœ¬..."
cat > npu_env_wrapper.sh << 'EOF'
#!/bin/bash

# NPUçŽ¯å¢ƒwrapper - ç¡®ä¿æ‰€æœ‰å­è¿›ç¨‹æ­£ç¡®ç»§æ‰¿çŽ¯å¢ƒ

# è®¾ç½®åŸºæœ¬çŽ¯å¢ƒå˜é‡
export ASCEND_HOME="/usr/local/Ascend/ascend-toolkit"

# æŸ¥æ‰¾æ­£ç¡®çš„set_env.shè·¯å¾„
POSSIBLE_SET_ENV=(
    "/usr/local/Ascend/ascend-toolkit/set_env.sh"
    "/usr/local/Ascend/ascend-toolkit/latest/set_env.sh"
    "/usr/local/Ascend/set_env.sh"
)

for env_path in "${POSSIBLE_SET_ENV[@]}"; do
    if [ -f "$env_path" ]; then
        echo "âœ… ä½¿ç”¨çŽ¯å¢ƒæ–‡ä»¶: $env_path"
        source "$env_path"
        export ASCEND_HOME="$(dirname "$env_path")"
        break
    fi
done

# å¼ºåˆ¶è®¾ç½®æ‰€æœ‰å¿…è¦çš„çŽ¯å¢ƒå˜é‡
export PATH="/usr/local/Ascend/ascend-toolkit/latest/bin:/usr/local/Ascend/ascend-toolkit/bin:$PATH"
export LD_LIBRARY_PATH="/usr/local/Ascend/ascend-toolkit/latest/lib64:/usr/local/Ascend/ascend-toolkit/lib64:$LD_LIBRARY_PATH"
export PYTHONPATH="/usr/local/Ascend/ascend-toolkit/latest/python/site-packages:/usr/local/Ascend/ascend-toolkit/python/site-packages:$PYTHONPATH"
export PYTHONPATH="/usr/local/Ascend/ascend-toolkit/latest/opp/built-in/op_impl/ai_core/tbe:/usr/local/Ascend/ascend-toolkit/opp/built-in/op_impl/ai_core/tbe:$PYTHONPATH"
export PYTHONPATH=".:$PYTHONPATH"

# NPUç‰¹å®šçŽ¯å¢ƒå˜é‡
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29525
export TORCH_COMPILE_DISABLE=1
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:64
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1

# æ‰§è¡Œä¼ å…¥çš„å‘½ä»¤
exec "$@"
EOF

chmod +x npu_env_wrapper.sh

# 3. åˆ›å»ºè®­ç»ƒè„šæœ¬wrapper
echo "3. åˆ›å»ºè®­ç»ƒè„šæœ¬wrapper..."
cat > wrapped_base_train.py << 'EOF'
#!/usr/bin/env python3

import os
import sys

# å¼ºåˆ¶è®¾ç½®çŽ¯å¢ƒå˜é‡ (Pythonçº§åˆ«)
os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')
os.environ.setdefault('PYTORCH_NPU_ALLOC_CONF', 'max_split_size_mb:64')

# æ·»åŠ å¿…è¦è·¯å¾„åˆ°sys.path
sys.path.insert(0, '.')
sys.path.insert(0, '/usr/local/Ascend/ascend-toolkit/latest/python/site-packages')
sys.path.insert(0, '/usr/local/Ascend/ascend-toolkit/python/site-packages')

print(f"âœ… PythonçŽ¯å¢ƒwrapper: PID={os.getpid()}, RANK={os.environ.get('LOCAL_RANK', 'N/A')}")

# å¯¼å…¥å¹¶æ‰§è¡ŒåŽŸå§‹è®­ç»ƒè„šæœ¬
import scripts.base_train
EOF

chmod +x wrapped_base_train.py

# 4. 4NPUä¼˜åŒ–å™¨è¡¥ä¸
echo "4. åˆ›å»º4NPUä¼˜åŒ–å™¨è¡¥ä¸..."
cat > temp_ultimate_patch.py << 'EOF'
import torch
from nanochat.gpt import GPT

def ultimate_4npu_optimizers(self, unembedding_lr=0.001, embedding_lr=0.01, matrix_lr=0.01, weight_decay=0.0):
    print("ðŸ’ª ç»ˆæž4NPU FineWebä¼˜åŒ–å™¨: å¼ºåŠ›çŽ¯å¢ƒé…ç½®")
    
    # èŽ·å–å‚æ•°åˆ†ç»„
    embedding_params = [p for n, p in self.named_parameters() if 'emb_tok' in n]
    other_params = [p for n, p in self.named_parameters() if 'emb_tok' not in n]
    
    opts = []
    
    # åµŒå…¥å±‚ä¼˜åŒ–å™¨
    if embedding_params:
        embedding_opt = torch.optim.AdamW(
            [{'params': embedding_params, 'lr': embedding_lr*0.3, 'initial_lr': embedding_lr*0.3}], 
            lr=embedding_lr*0.3, 
            weight_decay=weight_decay, 
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,
            fused=False,
            amsgrad=False
        )
        opts.append(embedding_opt)
        print(f"  âœ… åµŒå…¥å±‚ä¼˜åŒ–å™¨: lr={embedding_lr*0.3:.6f}, {len(embedding_params)}ä¸ªå‚æ•°")
    
    # å…¶ä»–å‚æ•°ä¼˜åŒ–å™¨
    if other_params:
        other_opt = torch.optim.AdamW(
            [{'params': other_params, 'lr': matrix_lr*0.3, 'initial_lr': matrix_lr*0.3}], 
            lr=matrix_lr*0.3, 
            weight_decay=0.0, 
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,
            fused=False,
            amsgrad=False
        )
        opts.append(other_opt)
        print(f"  âœ… å…¶ä»–å‚æ•°ä¼˜åŒ–å™¨: lr={matrix_lr*0.3:.6f}, {len(other_params)}ä¸ªå‚æ•°")
    
    print(f"  âœ… æ€»å…± {len(opts)} ä¸ªä¼˜åŒ–å™¨")
    return opts

# åº”ç”¨è¡¥ä¸
GPT.setup_optimizers = ultimate_4npu_optimizers
print("âœ… ç»ˆæž4NPU FineWebä¼˜åŒ–å™¨è¡¥ä¸å·²åº”ç”¨")
EOF

# 5. è®­ç»ƒtokenizer
echo "5. è®­ç»ƒtokenizer..."
if [ ! -f ~/.cache/nanochat/tokenizer/tokenizer.pkl ]; then
    echo "è®­ç»ƒtokenizer..."
    ./npu_env_wrapper.sh python -m scripts.tok_train
else
    echo "âœ… tokenizerå·²å­˜åœ¨"
fi

# 6. å¯åŠ¨ç»ˆæž4NPUè®­ç»ƒ
echo ""
echo "ðŸš€ å¯åŠ¨ç»ˆæž4NPU FineWebè®­ç»ƒ..."
echo ""
echo "ðŸ“Š è®­ç»ƒé…ç½®:"
echo "  - ç»ˆæž4NPUé…ç½® + wrapperçŽ¯å¢ƒ"
echo "  - æ¨¡åž‹æ·±åº¦: 8å±‚"
echo "  - æ‰¹æ¬¡å¤§å°: æ¯è®¾å¤‡2, æ€»32768"
echo "  - è®­ç»ƒæ­¥æ•°: 800æ­¥"
echo "  - çŽ¯å¢ƒ: å¤šå±‚wrapperä¿æŠ¤"
echo "  - é¢„è®¡æ—¶é—´: 25-45åˆ†é’Ÿ"
echo ""

# ä½¿ç”¨wrapperå¯åŠ¨è®­ç»ƒ
./npu_env_wrapper.sh python -c "import temp_ultimate_patch" && \
./npu_env_wrapper.sh torchrun --nproc_per_node=4 \
    --master_addr=127.0.0.1 \
    --master_port=29525 \
    wrapped_base_train.py \
    --model_tag=ultimate_4npu_fineweb_d8 \
    --depth=8 \
    --device_batch_size=2 \
    --total_batch_size=32768 \
    --num_iterations=800 \
    --embedding_lr=0.003 \
    --unembedding_lr=0.0003 \
    --matrix_lr=0.0015 \
    --grad_clip=0.4 \
    --eval_every=100 \
    --sample_every=400 \
    --core_metric_every=999999

# 7. æ¸…ç†
rm -f temp_ultimate_patch.py npu_env_wrapper.sh wrapped_base_train.py

echo ""
echo "ðŸŽ‰ ç»ˆæž4NPU FineWebè®­ç»ƒå®Œæˆï¼"
echo ""
echo "ðŸ“ æ¨¡åž‹ä½ç½®: ~/.cache/nanochat/base_checkpoints/ultimate_4npu_fineweb_d8/"
echo ""
echo "ðŸ’ª ç»ˆæžè§£å†³æ–¹æ¡ˆæˆåŠŸï¼"
