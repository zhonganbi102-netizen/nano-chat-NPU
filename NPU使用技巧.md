# NPUä½¿ç”¨æŠ€å·§å’Œæœ€ä½³å®è·µ

## ğŸ¯ æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥

### 1. å†…å­˜ç®¡ç†æœ€ä½³å®è·µ

```python
import torch_npu

# åœ¨è®­ç»ƒå¼€å§‹å‰æ¸…ç†å†…å­˜
torch_npu.npu.empty_cache()

# ç›‘æ§å†…å­˜ä½¿ç”¨
def print_memory_usage():
    allocated = torch_npu.npu.memory_allocated() / 1024**3
    cached = torch_npu.npu.memory_reserved() / 1024**3
    print(f"å†…å­˜: å·²åˆ†é… {allocated:.2f}GB, ç¼“å­˜ {cached:.2f}GB")

# åœ¨å…³é”®ç‚¹ç›‘æ§å†…å­˜
print_memory_usage()  # è®­ç»ƒå‰
# ... è®­ç»ƒä»£ç  ...
print_memory_usage()  # è®­ç»ƒå
```

### 2. æ‰¹æ¬¡å¤§å°åŠ¨æ€è°ƒæ•´

```python
# æ ¹æ®NPUå†…å­˜åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
def find_optimal_batch_size(model, max_batch_size=32):
    """è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜æ‰¹æ¬¡å¤§å°"""
    for batch_size in range(max_batch_size, 0, -1):
        try:
            # æµ‹è¯•å‰å‘ä¼ æ’­
            dummy_input = torch.randint(0, 1000, (batch_size, 512)).to('npu')
            with torch.no_grad():
                output = model(dummy_input)
            print(f"æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {batch_size}")
            return batch_size
        except RuntimeError as e:
            if "out of memory" in str(e):
                torch_npu.npu.empty_cache()
                continue
            else:
                raise e
    return 1

# ä½¿ç”¨ç¤ºä¾‹
optimal_batch_size = find_optimal_batch_size(model)
```

### 3. æ¢¯åº¦ç´¯ç§¯ç­–ç•¥

```python
# æ™ºèƒ½æ¢¯åº¦ç´¯ç§¯
def smart_gradient_accumulation(target_batch_size, device_batch_size):
    """è®¡ç®—æœ€ä¼˜æ¢¯åº¦ç´¯ç§¯æ­¥æ•°"""
    accumulation_steps = target_batch_size // device_batch_size
    effective_batch_size = device_batch_size * accumulation_steps
    
    print(f"ç›®æ ‡æ‰¹æ¬¡: {target_batch_size}")
    print(f"è®¾å¤‡æ‰¹æ¬¡: {device_batch_size}")
    print(f"ç´¯ç§¯æ­¥æ•°: {accumulation_steps}")
    print(f"æœ‰æ•ˆæ‰¹æ¬¡: {effective_batch_size}")
    
    return accumulation_steps

# ä½¿ç”¨ç¤ºä¾‹
accumulation_steps = smart_gradient_accumulation(
    target_batch_size=256,
    device_batch_size=32
)
```

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. æ•°æ®åŠ è½½ä¼˜åŒ–

```python
# ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨é…ç½®
def get_optimized_dataloader(dataset, batch_size):
    return torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=8,          # NPUæ¨è8ä¸ªworker
        pin_memory=True,        # å¯ç”¨å†…å­˜é”å®š
        persistent_workers=True, # ä¿æŒworkerè¿›ç¨‹
        prefetch_factor=4,      # é¢„å–å› å­
    )
```

### 2. æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–

```python
# NPUç‰¹å®šçš„ç¼–è¯‘ä¼˜åŒ–
def compile_model_for_npu(model):
    """ä¸ºNPUä¼˜åŒ–æ¨¡å‹ç¼–è¯‘"""
    # å¯ç”¨torch.compile (å¦‚æœæ”¯æŒ)
    try:
        compiled_model = torch.compile(
            model, 
            mode="reduce-overhead",  # NPUæ¨èæ¨¡å¼
            dynamic=False           # é™æ€å›¾ä¼˜åŒ–
        )
        print("âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸ")
        return compiled_model
    except Exception as e:
        print(f"âš ï¸ ç¼–è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹: {e}")
        return model

# ä½¿ç”¨ç¤ºä¾‹
model = compile_model_for_npu(model)
```

### 3. è‡ªåŠ¨æ··åˆç²¾åº¦ä¼˜åŒ–

```python
# NPUæ··åˆç²¾åº¦æœ€ä½³å®è·µ
def setup_mixed_precision():
    """é…ç½®NPUæ··åˆç²¾åº¦è®­ç»ƒ"""
    scaler = torch.npu.amp.GradScaler()
    
    # è®­ç»ƒå¾ªç¯ä¸­çš„ä½¿ç”¨
    def training_step(model, data, optimizer, scaler):
        with torch.npu.amp.autocast():
            output = model(data)
            loss = compute_loss(output)
        
        # æ¢¯åº¦ç¼©æ”¾
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
        
        return loss.item()
    
    return scaler, training_step
```

## ğŸ”§ è°ƒè¯•æŠ€å·§

### 1. NPUçŠ¶æ€æ£€æŸ¥å·¥å…·

```python
def npu_health_check():
    """NPUå¥åº·çŠ¶æ€æ£€æŸ¥"""
    import torch_npu
    
    print("=== NPUçŠ¶æ€æ£€æŸ¥ ===")
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"NPUå¯ç”¨: {torch_npu.npu.is_available()}")
    print(f"NPUæ•°é‡: {torch_npu.npu.device_count()}")
    print(f"å½“å‰è®¾å¤‡: {torch_npu.npu.current_device()}")
    
    # å†…å­˜ä¿¡æ¯
    for i in range(torch_npu.npu.device_count()):
        allocated = torch_npu.npu.memory_allocated(i) / 1024**3
        cached = torch_npu.npu.memory_reserved(i) / 1024**3
        print(f"NPU {i}: å·²ç”¨ {allocated:.2f}GB, ç¼“å­˜ {cached:.2f}GB")
    
    # æ¸©åº¦å’ŒåŠŸè€— (å¦‚æœæ”¯æŒ)
    try:
        import subprocess
        result = subprocess.run(['npu-smi', 'info'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("NPUè®¾å¤‡ä¿¡æ¯:")
            print(result.stdout)
    except:
        print("æ— æ³•è·å–NPUè®¾å¤‡è¯¦ç»†ä¿¡æ¯")

# åœ¨è®­ç»ƒå¼€å§‹å‰è¿è¡Œ
npu_health_check()
```

### 2. é”™è¯¯è¯Šæ–­å·¥å…·

```python
def diagnose_npu_error(error_msg):
    """NPUé”™è¯¯è¯Šæ–­åŠ©æ‰‹"""
    diagnostics = {
        "out of memory": [
            "å‡å°batch_size",
            "å¢åŠ gradient_accumulation_steps", 
            "ä½¿ç”¨gradient_checkpointing",
            "æ¸…ç†NPUç¼“å­˜: torch_npu.npu.empty_cache()"
        ],
        "HCCL": [
            "æ£€æŸ¥ç½‘ç»œè¿é€šæ€§",
            "éªŒè¯MASTER_ADDRå’ŒMASTER_PORT",
            "ç¡®è®¤WORLD_SIZEå’ŒRANKè®¾ç½®",
            "é‡å¯åˆ†å¸ƒå¼è®­ç»ƒ"
        ],
        "compile": [
            "æ£€æŸ¥CANNç‰ˆæœ¬å…¼å®¹æ€§",
            "éªŒè¯torch_npuç‰ˆæœ¬",
            "å°è¯•disable torch.compile",
            "æ£€æŸ¥ç®—å­æ”¯æŒæƒ…å†µ"
        ]
    }
    
    print(f"é”™è¯¯è¯Šæ–­: {error_msg}")
    for keyword, solutions in diagnostics.items():
        if keyword in error_msg.lower():
            print(f"\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ ({keyword}):")
            for i, solution in enumerate(solutions, 1):
                print(f"{i}. {solution}")
            break
    else:
        print("æœªæ‰¾åˆ°åŒ¹é…çš„è§£å†³æ–¹æ¡ˆï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æ—¥å¿—")

# ä½¿ç”¨ç¤ºä¾‹
try:
    # è®­ç»ƒä»£ç 
    pass
except Exception as e:
    diagnose_npu_error(str(e))
```

## ğŸ“Š æ€§èƒ½ç›‘æ§å·¥å…·

### 1. å®æ—¶æ€§èƒ½ç›‘æ§

```python
import time
import threading
from collections import deque

class NPUMonitor:
    """NPUæ€§èƒ½å®æ—¶ç›‘æ§"""
    
    def __init__(self, interval=1.0):
        self.interval = interval
        self.running = False
        self.metrics = {
            'memory_used': deque(maxlen=100),
            'memory_cached': deque(maxlen=100),
            'timestamps': deque(maxlen=100)
        }
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
        print("ğŸ” NPUç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join()
        print("â¹ï¸ NPUç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        import torch_npu
        
        while self.running:
            try:
                memory_used = torch_npu.npu.memory_allocated() / 1024**3
                memory_cached = torch_npu.npu.memory_reserved() / 1024**3
                timestamp = time.time()
                
                self.metrics['memory_used'].append(memory_used)
                self.metrics['memory_cached'].append(memory_cached)
                self.metrics['timestamps'].append(timestamp)
                
                time.sleep(self.interval)
            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
                break
    
    def get_summary(self):
        """è·å–ç›‘æ§æ‘˜è¦"""
        if not self.metrics['memory_used']:
            return "æ— ç›‘æ§æ•°æ®"
        
        avg_memory = sum(self.metrics['memory_used']) / len(self.metrics['memory_used'])
        max_memory = max(self.metrics['memory_used'])
        
        return f"""
NPUç›‘æ§æ‘˜è¦:
- å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory:.2f} GB
- å³°å€¼å†…å­˜ä½¿ç”¨: {max_memory:.2f} GB
- ç›‘æ§æ—¶é•¿: {len(self.metrics['memory_used'])} ç§’
        """

# ä½¿ç”¨ç¤ºä¾‹
monitor = NPUMonitor()
monitor.start_monitoring()

# è®­ç»ƒä»£ç 
try:
    # ... è®­ç»ƒå¾ªç¯ ...
    pass
finally:
    monitor.stop_monitoring()
    print(monitor.get_summary())
```

### 2. è®­ç»ƒé€Ÿåº¦åˆ†æ

```python
class TrainingProfiler:
    """è®­ç»ƒæ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.step_times = []
        self.throughput_history = []
    
    def start_step(self):
        """å¼€å§‹è®¡æ—¶ä¸€ä¸ªè®­ç»ƒæ­¥"""
        self.step_start = time.time()
    
    def end_step(self, num_tokens):
        """ç»“æŸè®¡æ—¶å¹¶è®°å½•ååé‡"""
        step_time = time.time() - self.step_start
        throughput = num_tokens / step_time
        
        self.step_times.append(step_time)
        self.throughput_history.append(throughput)
        
        return step_time, throughput
    
    def get_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.step_times:
            return "æ— æ€§èƒ½æ•°æ®"
        
        avg_time = sum(self.step_times) / len(self.step_times)
        avg_throughput = sum(self.throughput_history) / len(self.throughput_history)
        
        return f"""
è®­ç»ƒæ€§èƒ½ç»Ÿè®¡:
- å¹³å‡æ­¥æ—¶é—´: {avg_time:.3f} ç§’
- å¹³å‡ååé‡: {avg_throughput:.0f} tokens/ç§’
- æ€»æ­¥æ•°: {len(self.step_times)}
        """

# ä½¿ç”¨ç¤ºä¾‹
profiler = TrainingProfiler()

for step in range(num_steps):
    profiler.start_step()
    
    # è®­ç»ƒä»£ç 
    loss = training_step()
    
    step_time, throughput = profiler.end_step(num_tokens=batch_size * seq_len)
    
    if step % 10 == 0:
        print(f"Step {step}: {step_time:.3f}s, {throughput:.0f} tok/s")

print(profiler.get_stats())
```

## ğŸš€ ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–

### 1. æœåŠ¡ç¨³å®šæ€§

```python
import functools
import logging

def npu_error_retry(max_retries=3, delay=1.0):
    """NPUé”™è¯¯é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except RuntimeError as e:
                    if "NPU" in str(e) and attempt < max_retries - 1:
                        logging.warning(f"NPUé”™è¯¯ï¼Œé‡è¯• {attempt + 1}/{max_retries}: {e}")
                        torch_npu.npu.empty_cache()
                        time.sleep(delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
                        continue
                    raise
            return None
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@npu_error_retry(max_retries=3)
def inference_with_retry(model, input_text):
    """å¸¦é‡è¯•çš„æ¨ç†å‡½æ•°"""
    tokens = tokenizer.encode(input_text)
    with torch.no_grad():
        output = model.generate(tokens)
    return tokenizer.decode(output)
```

### 2. èµ„æºç®¡ç†

```python
class NPUResourceManager:
    """NPUèµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.models = {}
        self.memory_threshold = 0.9  # 90%å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼
    
    def load_model(self, model_name, model_path):
        """æ™ºèƒ½æ¨¡å‹åŠ è½½"""
        # æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
        if self._check_memory_pressure():
            self._cleanup_unused_models()
        
        # åŠ è½½æ¨¡å‹
        model = load_model_from_path(model_path)
        model = model.to('npu')
        self.models[model_name] = {
            'model': model,
            'last_used': time.time(),
            'usage_count': 0
        }
        
        return model
    
    def get_model(self, model_name):
        """è·å–æ¨¡å‹å¹¶æ›´æ–°ä½¿ç”¨ç»Ÿè®¡"""
        if model_name in self.models:
            self.models[model_name]['last_used'] = time.time()
            self.models[model_name]['usage_count'] += 1
            return self.models[model_name]['model']
        return None
    
    def _check_memory_pressure(self):
        """æ£€æŸ¥å†…å­˜å‹åŠ›"""
        import torch_npu
        allocated = torch_npu.npu.memory_allocated()
        total = torch_npu.npu.get_device_properties(0).total_memory
        return allocated / total > self.memory_threshold
    
    def _cleanup_unused_models(self):
        """æ¸…ç†æœªä½¿ç”¨çš„æ¨¡å‹"""
        current_time = time.time()
        to_remove = []
        
        for name, info in self.models.items():
            # 10åˆ†é’Ÿæœªä½¿ç”¨çš„æ¨¡å‹
            if current_time - info['last_used'] > 600:
                to_remove.append(name)
        
        for name in to_remove:
            del self.models[name]['model']
            del self.models[name]
            torch_npu.npu.empty_cache()
            print(f"æ¸…ç†æ¨¡å‹: {name}")

# ä½¿ç”¨ç¤ºä¾‹
resource_manager = NPUResourceManager()
model = resource_manager.load_model("chat_model", "/path/to/model")
```

è¿™äº›æŠ€å·§å’Œå·¥å…·å¯ä»¥å¸®åŠ©ä½ åœ¨åä¸ºæ˜‡è…¾NPUä¸Šæ›´å¥½åœ°è¿è¡Œnanochatï¼Œæå‡è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚è®°ä½è¦æ ¹æ®å…·ä½“çš„ç¡¬ä»¶é…ç½®å’Œåº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ï¼
