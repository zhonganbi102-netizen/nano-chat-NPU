#!/bin/bash

echo "=== ğŸ” NPUè®­ç»ƒè°ƒè¯•è„šæœ¬ ==="

# è®¾ç½®è°ƒè¯•ç¯å¢ƒå˜é‡
export ASCEND_GLOBAL_LOG_LEVEL=3  # è¯¦ç»†æ—¥å¿—
export ASCEND_SLOG_PRINT_TO_STDOUT=1  # è¾“å‡ºåˆ°æ§åˆ¶å°
export PYTHONPATH=/mnt/linxid615/bza/nanochat-npu:$PYTHONPATH

echo "1. æ£€æŸ¥NPUçŠ¶æ€..."
python3 -c "
try:
    import torch_npu
    print(f'âœ… torch_npuç‰ˆæœ¬: {torch_npu.__version__}')
    print(f'âœ… NPUè®¾å¤‡æ•°é‡: {torch_npu.npu.device_count()}')
    for i in range(torch_npu.npu.device_count()):
        print(f'   NPU {i}: {torch_npu.npu.get_device_name(i)}')
except Exception as e:
    print(f'âŒ NPUæ£€æŸ¥å¤±è´¥: {e}')
"

echo "2. æ£€æŸ¥æ•°æ®æ–‡ä»¶..."
python3 -c "
import sys
sys.path.insert(0, '/mnt/linxid615/bza/nanochat-npu')

try:
    from nanochat.dataset import list_parquet_files
    files = list_parquet_files()
    print(f'âœ… æ‰¾åˆ° {len(files)} ä¸ªæ•°æ®æ–‡ä»¶')
    if len(files) > 0:
        print(f'   ç¬¬ä¸€ä¸ªæ–‡ä»¶: {files[0]}')
        print(f'   æœ€åä¸€ä¸ªæ–‡ä»¶: {files[-1]}')
    else:
        print('âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼')
        exit(1)
except Exception as e:
    print(f'âŒ æ•°æ®æ£€æŸ¥å¤±è´¥: {e}')
    import traceback
    traceback.print_exc()
    exit(1)
"

echo "3. æµ‹è¯•tokenizer..."
python3 -c "
import sys
sys.path.insert(0, '/mnt/linxid615/bza/nanochat-npu')

try:
    from nanochat.tokenizer import get_tokenizer
    print('æ­£åœ¨åŠ è½½tokenizer...')
    tokenizer = get_tokenizer()
    vocab_size = tokenizer.get_vocab_size()
    print(f'âœ… TokenizeråŠ è½½æˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {vocab_size}')
    
    # æµ‹è¯•ç¼–ç 
    test_text = 'Hello world'
    tokens = tokenizer.encode([test_text])
    print(f'âœ… ç¼–ç æµ‹è¯•æˆåŠŸ: \"{test_text}\" -> {len(tokens[0])} tokens')
except Exception as e:
    print(f'âŒ Tokenizeræµ‹è¯•å¤±è´¥: {e}')
    import traceback
    traceback.print_exc()
    exit(1)
"

echo "4. æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆè¿™é‡Œé€šå¸¸ä¼šå¡ä½ï¼‰..."
timeout 30 python3 -c "
import sys
sys.path.insert(0, '/mnt/linxid615/bza/nanochat-npu')

try:
    print('å¯¼å…¥å¿…è¦æ¨¡å—...')
    import torch
    import torch_npu
    from nanochat.dataloader import tokenizing_distributed_data_loader
    from nanochat.common import get_dist_info
    
    print('âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ')
    
    print('è·å–åˆ†å¸ƒå¼ä¿¡æ¯...')
    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()
    print(f'âœ… åˆ†å¸ƒå¼è®¾ç½®: rank={ddp_rank}, world_size={ddp_world_size}')
    
    print('åˆ›å»ºæ•°æ®åŠ è½½å™¨...')
    train_loader = tokenizing_distributed_data_loader(
        B=2,  # å¾ˆå°çš„batch size
        T=128,  # å¾ˆå°çš„åºåˆ—é•¿åº¦
        split='train'
    )
    print('âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ')
    
    print('è·å–ç¬¬ä¸€æ‰¹æ•°æ®...')
    x, y = next(train_loader)
    print(f'âœ… æ•°æ®åŠ è½½æˆåŠŸ: x.shape={x.shape}, y.shape={y.shape}')
    print(f'   æ•°æ®è®¾å¤‡: {x.device}')
    
except Exception as e:
    print(f'âŒ æ•°æ®åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}')
    import traceback
    traceback.print_exc()
    exit(1)
" || echo "âŒ æ•°æ®åŠ è½½å™¨æµ‹è¯•è¶…æ—¶ï¼ˆ30ç§’ï¼‰"

echo "5. å¦‚æœä¸Šé¢éƒ½æˆåŠŸï¼Œå¼€å§‹è°ƒè¯•ç‰ˆè®­ç»ƒ..."

if [ $? -eq 0 ]; then
    echo "å¼€å§‹å¸¦è°ƒè¯•ä¿¡æ¯çš„è®­ç»ƒ..."
    
    python3 -c "
import sys
sys.path.insert(0, '/mnt/linxid615/bza/nanochat-npu')

print('=== ğŸš€ å¼€å§‹è°ƒè¯•ç‰ˆè®­ç»ƒ ===')

try:
    import os
    import time
    import torch
    import torch_npu
    
    print('Step 1: å¯¼å…¥nanochatæ¨¡å—...')
    from nanochat.gpt import GPT, GPTConfig
    from nanochat.dataloader import tokenizing_distributed_data_loader
    from nanochat.common import compute_init, print0
    from nanochat.tokenizer import get_tokenizer
    print('âœ… æ¨¡å—å¯¼å…¥å®Œæˆ')
    
    print('Step 2: è®¡ç®—åˆå§‹åŒ–...')
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init()
    print(f'âœ… è®¡ç®—åˆå§‹åŒ–å®Œæˆ: device={device}')
    
    print('Step 3: è®¾ç½®å‚æ•°...')
    depth = 12
    max_seq_len = 256  # å‡å°åºåˆ—é•¿åº¦
    device_batch_size = 4  # å‡å°batch size
    
    print('Step 4: åŠ è½½tokenizer...')
    tokenizer = get_tokenizer()
    vocab_size = tokenizer.get_vocab_size()
    print(f'âœ… TokenizeråŠ è½½å®Œæˆ: vocab_size={vocab_size}')
    
    print('Step 5: åˆ›å»ºæ¨¡å‹é…ç½®...')
    num_layers = depth
    model_dim = depth * 64
    num_heads = max(1, (model_dim + 127) // 128)
    num_kv_heads = num_heads
    
    model_config_kwargs = dict(
        sequence_len=max_seq_len, 
        vocab_size=vocab_size, 
        n_layer=num_layers, 
        n_head=num_heads, 
        n_kv_head=num_kv_heads, 
        n_embd=model_dim
    )
    print(f'âœ… æ¨¡å‹é…ç½®: layers={num_layers}, dim={model_dim}, heads={num_heads}')
    
    print('Step 6: åˆ›å»ºæ¨¡å‹...')
    with torch.device('meta'):
        model_config = GPTConfig(**model_config_kwargs)
        model = GPT(model_config)
    
    model.to_empty(device=device)
    model.init_weights()
    num_params = sum(p.numel() for p in model.parameters())
    print(f'âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ: {num_params:,} å‚æ•°')
    
    print('Step 7: åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå…³é”®æ­¥éª¤ï¼‰...')
    train_loader = tokenizing_distributed_data_loader(
        device_batch_size, max_seq_len, split='train'
    )
    print('âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ')
    
    print('Step 8: è·å–ç¬¬ä¸€æ‰¹æ•°æ®...')
    x, y = next(train_loader)
    print(f'âœ… ç¬¬ä¸€æ‰¹æ•°æ®è·å–æˆåŠŸ: {x.shape}, {y.shape}')
    
    print('Step 9: æµ‹è¯•å‰å‘ä¼ æ’­...')
    model.train()
    with torch.amp.autocast(device_type='npu', dtype=torch.bfloat16):
        loss = model(x, y)
    print(f'âœ… å‰å‘ä¼ æ’­æˆåŠŸ: loss={loss.item():.4f}')
    
    print('\\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç°åœ¨å¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒäº†ï¼')
    
except Exception as e:
    print(f'âŒ è°ƒè¯•è®­ç»ƒå¤±è´¥: {e}')
    import traceback
    traceback.print_exc()
"
else
    echo "âŒ å‰é¢çš„æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ"
    echo ""
    echo "ğŸ” æ•…éšœæ’é™¤å»ºè®®:"
    echo "1. æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨: ls -la base_data/*.parquet"
    echo "2. æ£€æŸ¥NPUçŠ¶æ€: npu-smi info"
    echo "3. é‡å¯NPUç¯å¢ƒ: source clean_npu_environment.sh"
    echo "4. æ£€æŸ¥å†…å­˜ä½¿ç”¨: free -h"
fi