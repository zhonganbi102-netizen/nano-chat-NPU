#!/bin/bash

echo "ğŸš€ NPUå…¼å®¹çš„4NPUæµ‹è¯•..."

# ç¯å¢ƒå˜é‡è®¾ç½®
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500

# HCCLè®¾ç½®
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1

# ç§»é™¤CUDAæ ¼å¼çš„å†…å­˜é…ç½®
unset PYTORCH_NPU_ALLOC_CONF

# æ¸…ç†
pkill -f "python.*base_train.py" 2>/dev/null || true
pkill -f "torchrun" 2>/dev/null || true
sleep 3

echo "é…ç½®ä¿¡æ¯ï¼š"
echo "  ğŸ”§ å·²ä¿®å¤ä¼˜åŒ–å™¨ï¼šä½¿ç”¨æ ‡å‡†AdamWå’ŒMuon"
echo "  ğŸ’¾ NPUåŸç”Ÿå†…å­˜ç®¡ç†"
echo "  ğŸ¯ 4ä¸ªNPU (0,1,2,3)"
echo "  ğŸ“Š å°æ¨¡å‹ï¼šdepth=4"
echo "  ğŸ’¾ å°batchï¼šdevice_batch_size=2"
echo ""

echo "å¯åŠ¨NPUå…¼å®¹çš„4NPUæµ‹è¯•..."

torchrun --standalone --nproc_per_node=4 -- scripts/base_train.py \
    --depth=4 \
    --device_batch_size=2 \
    --total_batch_size=16384 \
    --max_seq_len=512 \
    --num_iterations=10 \
    --eval_every=999999 \
    --core_metric_every=999999 \
    --sample_every=999999 \
    --run="clean_4npu_$(date +%Y%m%d_%H%M%S)"

if [ $? -eq 0 ]; then
    echo ""
    echo "ğŸ‰ NPUå…¼å®¹4NPUæµ‹è¯•æˆåŠŸï¼"
    echo "4NPUåˆ†å¸ƒå¼è®­ç»ƒå·²å®Œå…¨æ­£å¸¸ï¼"
else
    echo ""
    echo "âŒ NPUå…¼å®¹4NPUæµ‹è¯•å¤±è´¥"
fi

echo ""
echo "NPUå…¼å®¹4NPUæµ‹è¯•å®Œæˆ: $(date)"