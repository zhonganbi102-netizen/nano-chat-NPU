#!/bin/bash

echo "ğŸ¯ æ— åˆå§‹è¯„ä¼°çš„2NPUè®­ç»ƒ..."

# åŸºç¡€ç¯å¢ƒå˜é‡
export ASCEND_RT_VISIBLE_DEVICES=0,1
export WORLD_SIZE=2
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500

# HCCLè®¾ç½®
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1

# å†…å­˜è®¾ç½®
export PYTORCH_NPU_ALLOC_CONF="max_split_size_mb:256"

# æ¸…ç†
pkill -f "python.*base_train.py" 2>/dev/null || true
pkill -f "torchrun" 2>/dev/null || true
sleep 3

echo "åŸºç¡€åˆ†å¸ƒå¼é€šä¿¡å·²éªŒè¯æˆåŠŸâœ…"
echo "ç°åœ¨æµ‹è¯•æ— åˆå§‹è¯„ä¼°çš„2NPUè®­ç»ƒ..."
echo ""
echo "é…ç½®:"
echo "  - è·³è¿‡åˆå§‹éªŒè¯è¯„ä¼°"
echo "  - ä½¿ç”¨ä¸å•NPUæˆåŠŸé…ç½®ç±»ä¼¼çš„å‚æ•°"
echo "  - depth=4, batch=2"

# å‚è€ƒæˆåŠŸçš„å•NPUé…ç½®ï¼Œä½†ä½¿ç”¨2NPUåˆ†å¸ƒå¼
torchrun --standalone --nproc_per_node=2 -- scripts/base_train.py \
    --depth=4 \
    --device_batch_size=2 \
    --total_batch_size=16384 \
    --max_seq_len=512 \
    --num_iterations=10 \
    --eval_every=999999 \
    --core_metric_every=999999 \
    --sample_every=999999 \
    --run="no_eval_2npu_$(date +%Y%m%d_%H%M%S)"

if [ $? -eq 0 ]; then
    echo ""
    echo "ğŸ‰ 2NPUè®­ç»ƒæˆåŠŸï¼ç°åœ¨å¯ä»¥å°è¯•4NPU"
    echo "æ¨èä¸‹ä¸€æ­¥è¿è¡Œ: ./working_4npu.sh"
else
    echo ""
    echo "âŒ 2NPUè®­ç»ƒå¤±è´¥"
    echo "å»ºè®®è¿è¡Œ: ./debug_2npu.sh è·å–è¯¦ç»†æ—¥å¿—"
fi

echo ""
echo "2NPUæµ‹è¯•å®Œæˆ: $(date)"