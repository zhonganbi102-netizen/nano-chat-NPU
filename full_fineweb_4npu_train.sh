#!/bin/bash

# å®Œæ•´FineWebæ•°æ®é›†4NPUå¤§è§„æ¨¡è®­ç»ƒ
# åŸºäºŽå•NPUæˆåŠŸç»éªŒçš„4NPUå®Œæ•´ç‰ˆæœ¬

set -e

echo "ðŸ”¥ å®Œæ•´FineWebæ•°æ®é›†4NPUå¤§è§„æ¨¡è®­ç»ƒ ðŸ”¥"

# 1. å¼ºåŠ›æ¸…ç†
echo "1. å¼ºåŠ›æ¸…ç†NPUçŽ¯å¢ƒ..."
./emergency_npu_cleanup.sh
sleep 20

# 2. éªŒè¯æ•°æ®é›†
data_files=$(ls base_data/shard_*.parquet 2>/dev/null | wc -l || echo "0")
if [ "$data_files" -lt 100 ]; then
    echo "âŒ FineWebæ•°æ®æ–‡ä»¶ä¸è¶³($data_filesä¸ª)ï¼Œå»ºè®®è‡³å°‘100ä¸ªæ–‡ä»¶"
    echo "è¯·è¿è¡Œ: ./download_fineweb_data.sh"
    exit 1
fi
echo "âœ… FineWebæ•°æ®æ–‡ä»¶: $data_files ä¸ª (~$(($data_files * 150))MB)"

# 3. åˆ›å»ºçŽ¯å¢ƒwrapperè„šæœ¬ (åŸºäºŽæˆåŠŸç»éªŒ)
echo "2. åˆ›å»ºå®Œæ•´è®­ç»ƒçŽ¯å¢ƒwrapper..."
cat > full_npu_env_wrapper.sh << 'EOF'
#!/bin/bash

# å®Œæ•´è®­ç»ƒNPUçŽ¯å¢ƒwrapper - åŸºäºŽå•NPUæˆåŠŸç»éªŒ

echo "ðŸš€ è®¾ç½®å®Œæ•´FineWebè®­ç»ƒçŽ¯å¢ƒ..."

# æŸ¥æ‰¾å¹¶è®¾ç½®AscendçŽ¯å¢ƒ
ASCEND_PATHS=(
    "/usr/local/Ascend/ascend-toolkit/set_env.sh"
    "/usr/local/Ascend/ascend-toolkit/latest/set_env.sh"
    "/usr/local/Ascend/set_env.sh"
)

for env_path in "${ASCEND_PATHS[@]}"; do
    if [ -f "$env_path" ]; then
        echo "âœ… ä½¿ç”¨çŽ¯å¢ƒæ–‡ä»¶: $env_path"
        source "$env_path"
        export ASCEND_HOME="$(dirname "$env_path")"
        break
    fi
done

# å¼ºåˆ¶è®¾ç½®æ‰€æœ‰å¿…è¦çŽ¯å¢ƒå˜é‡
export PATH="/usr/local/Ascend/ascend-toolkit/latest/bin:/usr/local/Ascend/ascend-toolkit/bin:$PATH"
export LD_LIBRARY_PATH="/usr/local/Ascend/ascend-toolkit/latest/lib64:/usr/local/Ascend/ascend-toolkit/lib64:$LD_LIBRARY_PATH"
export PYTHONPATH="/usr/local/Ascend/ascend-toolkit/latest/python/site-packages:/usr/local/Ascend/ascend-toolkit/python/site-packages:$PYTHONPATH"
export PYTHONPATH="/usr/local/Ascend/ascend-toolkit/latest/opp/built-in/op_impl/ai_core/tbe:/usr/local/Ascend/ascend-toolkit/opp/built-in/op_impl/ai_core/tbe:$PYTHONPATH"
export PYTHONPATH=".:$PYTHONPATH"

# 4NPUåˆ†å¸ƒå¼çŽ¯å¢ƒå˜é‡
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29526
export TORCH_COMPILE_DISABLE=1
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:128  # æ›´å¤§å†…å­˜é…ç½®ç”¨äºŽå®Œæ•´è®­ç»ƒ
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1

echo "âœ… å®Œæ•´è®­ç»ƒçŽ¯å¢ƒè®¾ç½®å®Œæˆ"

# æ‰§è¡Œä¼ å…¥çš„å‘½ä»¤
exec "$@"
EOF

chmod +x full_npu_env_wrapper.sh

# 4. å®Œæ•´è®­ç»ƒPython wrapper
echo "3. åˆ›å»ºå®Œæ•´è®­ç»ƒè„šæœ¬wrapper..."
cat > full_base_train.py << 'EOF'
#!/usr/bin/env python3

import os
import sys

# åŸºäºŽå•NPUæˆåŠŸç»éªŒçš„çŽ¯å¢ƒè®¾ç½®
os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')
os.environ.setdefault('PYTORCH_NPU_ALLOC_CONF', 'max_split_size_mb:128')

# æ·»åŠ å¿…è¦è·¯å¾„
sys.path.insert(0, '.')
sys.path.insert(0, '/usr/local/Ascend/ascend-toolkit/latest/python/site-packages')
sys.path.insert(0, '/usr/local/Ascend/ascend-toolkit/python/site-packages')

print(f"ðŸ”¥ å®Œæ•´FineWebè®­ç»ƒwrapper: PID={os.getpid()}, RANK={os.environ.get('LOCAL_RANK', 'N/A')}")

# å¯¼å…¥å¹¶æ‰§è¡ŒåŽŸå§‹è®­ç»ƒè„šæœ¬
import scripts.base_train
EOF

chmod +x full_base_train.py

# 5. å®Œæ•´è®­ç»ƒä¼˜åŒ–å™¨è¡¥ä¸ (åŸºäºŽå•NPUæˆåŠŸé…ç½®)
echo "4. åˆ›å»ºå®Œæ•´è®­ç»ƒä¼˜åŒ–å™¨è¡¥ä¸..."
cat > temp_full_train_patch.py << 'EOF'
import torch
from nanochat.gpt import GPT

def full_fineweb_optimizers(self, unembedding_lr=0.001, embedding_lr=0.01, matrix_lr=0.01, weight_decay=0.0):
    print("ðŸ”¥ å®Œæ•´FineWebæ•°æ®é›†4NPUè®­ç»ƒä¼˜åŒ–å™¨")
    
    # åŸºäºŽå•NPUæˆåŠŸç»éªŒçš„å‚æ•°åˆ†ç»„
    embedding_params = [p for n, p in self.named_parameters() if 'emb_tok' in n]
    other_params = [p for n, p in self.named_parameters() if 'emb_tok' not in n]
    
    opts = []
    
    # åµŒå…¥å±‚ä¼˜åŒ–å™¨ (åŸºäºŽå•NPUæˆåŠŸé…ç½®è°ƒæ•´)
    if embedding_params:
        embedding_opt = torch.optim.AdamW(
            [{'params': embedding_params, 'lr': embedding_lr*0.5, 'initial_lr': embedding_lr*0.5}], 
            lr=embedding_lr*0.5, 
            weight_decay=weight_decay, 
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,  # å…³é”®: åŸºäºŽå•NPUæˆåŠŸç»éªŒ
            fused=False,
            amsgrad=False
        )
        opts.append(embedding_opt)
        print(f"  âœ… åµŒå…¥å±‚ä¼˜åŒ–å™¨: lr={embedding_lr*0.5:.6f}, {len(embedding_params)}ä¸ªå‚æ•°")
    
    # å…¶ä»–å‚æ•°ä¼˜åŒ–å™¨
    if other_params:
        other_opt = torch.optim.AdamW(
            [{'params': other_params, 'lr': matrix_lr*0.5, 'initial_lr': matrix_lr*0.5}], 
            lr=matrix_lr*0.5, 
            weight_decay=0.0, 
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,  # å…³é”®: åŸºäºŽå•NPUæˆåŠŸç»éªŒ
            fused=False,
            amsgrad=False
        )
        opts.append(other_opt)
        print(f"  âœ… å…¶ä»–å‚æ•°ä¼˜åŒ–å™¨: lr={matrix_lr*0.5:.6f}, {len(other_params)}ä¸ªå‚æ•°")
    
    print(f"  ðŸ”¥ å®Œæ•´è®­ç»ƒæ€»å…± {len(opts)} ä¸ªä¼˜åŒ–å™¨")
    return opts

# åº”ç”¨è¡¥ä¸
GPT.setup_optimizers = full_fineweb_optimizers
print("âœ… å®Œæ•´FineWebæ•°æ®é›†4NPUè®­ç»ƒä¼˜åŒ–å™¨è¡¥ä¸å·²åº”ç”¨")
EOF

# 6. è®­ç»ƒtokenizer
echo "5. è®­ç»ƒtokenizer..."
if [ ! -f ~/.cache/nanochat/tokenizer/tokenizer.pkl ]; then
    echo "è®­ç»ƒtokenizer..."
    ./full_npu_env_wrapper.sh python -m scripts.tok_train
else
    echo "âœ… tokenizerå·²å­˜åœ¨"
fi

# 7. å®Œæ•´FineWebæ•°æ®é›†4NPUå¤§è§„æ¨¡è®­ç»ƒ
echo ""
echo "ðŸ”¥ å¯åŠ¨å®Œæ•´FineWebæ•°æ®é›†4NPUå¤§è§„æ¨¡è®­ç»ƒ..."
echo ""
echo "ðŸ“Š å®Œæ•´è®­ç»ƒé…ç½®:"
echo "  - æ•°æ®æ–‡ä»¶: $data_files ä¸ª (~$(($data_files * 150))MB)"
echo "  - å®Œæ•´FineWebæ•°æ®é›†è®­ç»ƒ"
echo "  - æ¨¡åž‹æ·±åº¦: 12å±‚ (å¤§æ¨¡åž‹)"
echo "  - 4NPUåˆ†å¸ƒå¼å¹¶è¡Œ"
echo "  - æ‰¹æ¬¡å¤§å°: æ¯è®¾å¤‡8, æ€»131072"
echo "  - è®­ç»ƒæ­¥æ•°: 4000æ­¥ (å……åˆ†è®­ç»ƒ)"
echo "  - å­¦ä¹ çŽ‡: åŸºäºŽå•NPUæˆåŠŸç»éªŒè°ƒæ•´"
echo "  - é¢„è®¡æ—¶é—´: 2-3å°æ—¶"
echo ""

# å¯åŠ¨å®Œæ•´å¤§è§„æ¨¡è®­ç»ƒ
./full_npu_env_wrapper.sh python -c "import temp_full_train_patch" && \
./full_npu_env_wrapper.sh torchrun --nproc_per_node=4 \
    --master_addr=127.0.0.1 \
    --master_port=29526 \
    full_base_train.py \
    --model_tag=full_fineweb_dataset_d12 \
    --depth=12 \
    --device_batch_size=8 \
    --total_batch_size=131072 \
    --num_iterations=4000 \
    --embedding_lr=0.01 \
    --unembedding_lr=0.001 \
    --matrix_lr=0.005 \
    --grad_clip=0.8 \
    --eval_every=200 \
    --sample_every=800 \
    --core_metric_every=999999

# 8. æ¸…ç†
rm -f temp_full_train_patch.py full_npu_env_wrapper.sh full_base_train.py

echo ""
echo "ðŸŽ‰ å®Œæ•´FineWebæ•°æ®é›†4NPUå¤§è§„æ¨¡è®­ç»ƒå®Œæˆï¼"
echo ""
echo "ðŸ“ æ¨¡åž‹ä½ç½®: ~/.cache/nanochat/base_checkpoints/full_fineweb_dataset_d12/"
echo ""
echo "ðŸ”¥ æ­å–œå®Œæˆå®Œæ•´æ•°æ®é›†è®­ç»ƒï¼"
echo "ðŸ“Š è®­ç»ƒç»Ÿè®¡:"
echo "  - æ€»è®­ç»ƒtoken: ~5.24äº¿tokens"
echo "  - æ¨¡åž‹å‚æ•°: ~337M"
echo "  - è®­ç»ƒè´¨é‡: å®Œæ•´FineWebæ•°æ®é›†"
