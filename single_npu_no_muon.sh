#!/bin/bash

# å•NPUè®­ç»ƒè„šæœ¬ - å®Œå…¨é¿å…Muonä¼˜åŒ–å™¨
# Single NPU training script - completely avoid Muon optimizer

set -e

echo "ðŸš€ å•NPUè®­ç»ƒ - æ— Muonç‰ˆæœ¬"
echo ""

# 1. çŽ¯å¢ƒè®¾ç½®
echo "1. è®¾ç½®å•NPUçŽ¯å¢ƒ..."
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# å•NPUçŽ¯å¢ƒå˜é‡
export ASCEND_RT_VISIBLE_DEVICES=0
export WORLD_SIZE=1
export RANK=0
export LOCAL_RANK=0
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29800

# NPUä¼˜åŒ–è®¾ç½®
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:128
export NPU_COMPILE_DISABLE=1
export TORCH_NPU_DISABLE_LAZY_INIT=1
export OMP_NUM_THREADS=1

echo "âœ… çŽ¯å¢ƒé…ç½®å®Œæˆ"

# 2. æ¸…ç†NPU
echo ""
echo "2. æ¸…ç†NPUçŽ¯å¢ƒ..."
pkill -f "python.*train" || true
pkill -f "torchrun" || true

python3 -c "
import torch
import torch_npu
import gc
if torch_npu.npu.is_available():
    torch_npu.npu.empty_cache()
    gc.collect()
    print('âœ… NPUç¼“å­˜å·²æ¸…ç†')
"

# 3. åˆ›å»ºæ— Muonä¼˜åŒ–å™¨è¡¥ä¸
echo ""
echo "3. åˆ›å»ºæ— Muonä¼˜åŒ–å™¨è¡¥ä¸..."
cat > single_npu_no_muon_patch.py << 'EOF'
import torch
from nanochat.gpt import GPT

def single_npu_adamw_optimizers(self, unembedding_lr=0.001, embedding_lr=0.01, matrix_lr=0.01, weight_decay=0.0):
    """
    å•NPUä¸“ç”¨AdamWä¼˜åŒ–å™¨ - å®Œå…¨æ›¿ä»£Muon
    """
    print("ðŸ”§ å•NPU: ä½¿ç”¨çº¯AdamWä¼˜åŒ–å™¨ï¼ˆæ— Muonï¼‰")
    
    # èŽ·å–å‚æ•°
    all_params = list(self.parameters())
    print(f"æ€»å‚æ•°æ•°é‡: {len(all_params)}")
    print(f"æ€»å‚æ•°é‡: {sum(p.numel() for p in all_params):,}")
    
    # ç®€åŒ–å‚æ•°åˆ†ç»„
    embedding_params = []
    other_params = []
    
    for name, param in self.named_parameters():
        if 'emb_tok' in name:
            embedding_params.append(param)
        else:
            other_params.append(param)
    
    print(f"Embeddingå‚æ•°: {len(embedding_params)}")
    print(f"å…¶ä»–å‚æ•°: {len(other_params)}")
    
    optimizers = []
    
    # åªä½¿ç”¨æ ‡å‡†AdamWï¼Œå®Œå…¨é¿å…å¤æ‚ä¼˜åŒ–å™¨
    if embedding_params:
        emb_opt = torch.optim.AdamW(
            embedding_params,
            lr=embedding_lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,
            fused=False
        )
        optimizers.append(emb_opt)
        print(f"âœ… Embedding AdamW: lr={embedding_lr}")
    
    if other_params:
        other_opt = torch.optim.AdamW(
            other_params,
            lr=matrix_lr,
            weight_decay=0.0,
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,
            fused=False
        )
        optimizers.append(other_opt)
        print(f"âœ… Other AdamW: lr={matrix_lr}")
    
    print(f"ðŸŽ¯ åˆ›å»ºäº† {len(optimizers)} ä¸ªAdamWä¼˜åŒ–å™¨")
    return optimizers

# æ›¿æ¢ä¼˜åŒ–å™¨æ–¹æ³•
GPT.setup_optimizers = single_npu_adamw_optimizers
print("âœ… å•NPUæ— Muonä¼˜åŒ–å™¨è¡¥ä¸å·²åº”ç”¨")
EOF

# 4. å¼€å§‹è®­ç»ƒ
echo ""
echo "4. å¼€å§‹å•NPUè®­ç»ƒï¼ˆæ— Muonï¼‰..."
echo "é…ç½®:"
echo "  - æ¨¡åž‹æ·±åº¦: 6å±‚"
echo "  - æ‰¹æ¬¡å¤§å°: 4"
echo "  - è®­ç»ƒæ­¥æ•°: 500æ­¥"
echo "  - ä¼˜åŒ–å™¨: çº¯AdamW"
echo ""

python3 -c "import single_npu_no_muon_patch" && \
python3 -m scripts.base_train \
    --run=single_npu_no_muon \
    --depth=6 \
    --device_batch_size=4 \
    --total_batch_size=4 \
    --num_iterations=500 \
    --embedding_lr=0.01 \
    --unembedding_lr=0.001 \
    --matrix_lr=0.005 \
    --grad_clip=1.0 \
    --eval_every=100 \
    --sample_every=200 \
    --core_metric_every=999999 \
    --verbose

# 5. æ¸…ç†
rm -f single_npu_no_muon_patch.py

echo ""
echo "ðŸŽ‰ å•NPUæ— Muonè®­ç»ƒå®Œæˆï¼"
echo "å¦‚æžœæˆåŠŸï¼Œå¯ä»¥å°è¯•å¢žåŠ å‚æ•°ï¼š"
echo "  bash conservative_fineweb_train.sh  # 4NPUç‰ˆæœ¬"