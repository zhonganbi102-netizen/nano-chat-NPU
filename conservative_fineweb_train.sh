#!/bin/bash

# ä¿å®ˆå†…å­˜é…ç½®çš„FineWebè®­ç»ƒè„šæœ¬
# ä¸“é—¨é’ˆå¯¹NPUå†…å­˜é™åˆ¶ä¼˜åŒ–

set -e

echo "=== ä¿å®ˆå†…å­˜FineWebè®­ç»ƒ ==="

# 1. å¼ºåŠ›æ¸…ç†NPUçŽ¯å¢ƒ
echo "1. å¼ºåŠ›æ¸…ç†NPUå†…å­˜..."
if [ -f "./emergency_npu_cleanup.sh" ]; then
    ./emergency_npu_cleanup.sh
    sleep 10
fi

# 2. è®¾ç½®çŽ¯å¢ƒ
echo "2. è®¾ç½®NPUçŽ¯å¢ƒ..."
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# 3. ä¿å®ˆçš„4NPUçŽ¯å¢ƒå˜é‡
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29521
export TORCH_COMPILE_DISABLE=1
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:64  # NPUå®‰å…¨çš„å†…å­˜åˆ†å‰²é…ç½®
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1
export NPU_CALCULATE_DEVICE=0,1,2,3

# 4. åˆ›å»ºå†…å­˜å‹å¥½çš„ä¼˜åŒ–å™¨è¡¥ä¸
echo "3. åˆ›å»ºå†…å­˜å‹å¥½ä¼˜åŒ–å™¨è¡¥ä¸..."
cat > temp_conservative_patch.py << EOF
import torch
from nanochat.gpt import GPT

def conservative_optimizers(self, unembedding_lr=0.001, embedding_lr=0.01, matrix_lr=0.01, weight_decay=0.0):
    print("ä¿å®ˆå†…å­˜ä¼˜åŒ–å™¨: AdamW + ä½Žå†…å­˜é…ç½®")
    embedding_params = [p for n, p in self.named_parameters() if 'emb_tok' in n]
    other_params = [p for n, p in self.named_parameters() if 'emb_tok' not in n]
    opts = []
    
    # ä½¿ç”¨æ›´ä¿å®ˆçš„å­¦ä¹ çŽ‡
    if embedding_params:
        opts.append(torch.optim.AdamW(
            [{'params': embedding_params, 'lr': embedding_lr*0.5, 'initial_lr': embedding_lr*0.5}], 
            lr=embedding_lr*0.5, 
            weight_decay=weight_decay, 
            betas=(0.9, 0.95),
            eps=1e-6,
            foreach=False  # å…³é—­foreachä¼˜åŒ–ä»¥èŠ‚çœå†…å­˜
        ))
    
    if other_params:
        opts.append(torch.optim.AdamW(
            [{'params': other_params, 'lr': matrix_lr*0.5, 'initial_lr': matrix_lr*0.5}], 
            lr=matrix_lr*0.5, 
            weight_decay=0.0, 
            betas=(0.9, 0.95),
            eps=1e-6,
            foreach=False  # å…³é—­foreachä¼˜åŒ–ä»¥èŠ‚çœå†…å­˜
        ))
    
    return opts

GPT.setup_optimizers = conservative_optimizers
print("âœ… ä¿å®ˆå†…å­˜ä¼˜åŒ–å™¨è¡¥ä¸å·²åº”ç”¨")
EOF

# 5. è®­ç»ƒtokenizer
echo "4. è®­ç»ƒtokenizer..."
if [ ! -f ~/.cache/nanochat/tokenizer/tokenizer.pkl ]; then
    python -m scripts.tok_train
else
    echo "tokenizerå·²å­˜åœ¨"
fi

# 6. ä¿å®ˆå†…å­˜è®­ç»ƒé…ç½®
echo "5. å¼€å§‹ä¿å®ˆå†…å­˜FineWebè®­ç»ƒ..."
echo ""
echo "ä¿å®ˆé…ç½®:"
echo "  - æ¨¡åž‹æ·±åº¦: 8å±‚ (é™ä½Žå†…å­˜)"
echo "  - æ‰¹æ¬¡å¤§å°: æ¯è®¾å¤‡4, æ€»å…±65536"
echo "  - è®­ç»ƒæ­¥æ•°: 2000"
echo "  - å†…å­˜ä¼˜åŒ–: å¯ç”¨"
echo "  - é¢„è®¡æ—¶é—´: 1-2å°æ—¶"
echo ""

python -c "import temp_conservative_patch" && \
PYTHONPATH=. torchrun --nproc_per_node=4 \
    --master_addr=127.0.0.1 \
    --master_port=29521 \
    scripts/base_train.py \
    --model_tag=fineweb_conservative_d8 \
    --depth=8 \
    --device_batch_size=4 \
    --total_batch_size=65536 \
    --num_iterations=2000 \
    --embedding_lr=0.1 \
    --unembedding_lr=0.002 \
    --matrix_lr=0.01 \
    --grad_clip=0.5 \
    --eval_every=200 \
    --sample_every=800 \
    --core_metric_every=999999

# 7. æ¸…ç†
rm -f temp_conservative_patch.py

echo ""
echo "ðŸŽ‰ ä¿å®ˆå†…å­˜FineWebè®­ç»ƒå®Œæˆï¼"
echo "æ¨¡åž‹ä¿å­˜: ~/.cache/nanochat/base_checkpoints/fineweb_conservative_d8/"
