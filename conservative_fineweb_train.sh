#!/bin/bash

# ä¿å®ˆå†…å­˜é…ç½®çš„FineWebè®­ç»ƒè„šæœ¬
# ä¸“é—¨é’ˆå¯¹NPUå†…å­˜é™åˆ¶ä¼˜åŒ–

set -e

echo "=== ä¿å®ˆå†…å­˜FineWebè®­ç»ƒ ==="

# 1. å¼ºåŠ›æ¸…ç†NPUçŽ¯å¢ƒ
echo "1. å¼ºåŠ›æ¸…ç†NPUå†…å­˜..."
if [ -f "./emergency_npu_cleanup.sh" ]; then
    ./emergency_npu_cleanup.sh
    sleep 10
fi

# 2. è®¾ç½®çŽ¯å¢ƒ
echo "2. è®¾ç½®NPUçŽ¯å¢ƒ..."
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# 3. ä¿å®ˆçš„4NPUçŽ¯å¢ƒå˜é‡
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29521
export TORCH_COMPILE_DISABLE=1
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:64  # NPUå®‰å…¨çš„å†…å­˜åˆ†å‰²é…ç½®
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1
export NPU_CALCULATE_DEVICE=0,1,2,3

# 4. åˆ›å»ºNPUå…¼å®¹çš„ä¼˜åŒ–å™¨è¡¥ä¸ï¼ˆå®Œå…¨é¿å…Muonï¼‰
echo "3. åˆ›å»ºNPUå…¼å®¹ä¼˜åŒ–å™¨è¡¥ä¸..."
cat > npu_adamw_patch.py << EOF
import torch
from nanochat.gpt import GPT

def npu_compatible_optimizers(self, unembedding_lr=0.001, embedding_lr=0.01, matrix_lr=0.01, weight_decay=0.0):
    """
    NPUå…¼å®¹çš„ä¼˜åŒ–å™¨ - å®Œå…¨æ›¿ä»£Muon
    ä½¿ç”¨æ ‡å‡†AdamWï¼Œé¿å…æ‰€æœ‰NPUä¸å…¼å®¹çš„ä¼˜åŒ–å™¨
    """
    print("ðŸ”§ ä½¿ç”¨NPUå…¼å®¹çš„AdamWä¼˜åŒ–å™¨ï¼ˆæ›¿ä»£Muonï¼‰")
    
    # èŽ·å–æ‰€æœ‰å‚æ•°å¹¶åˆ†ç»„
    embedding_params = []
    unembedding_params = []
    matrix_params = []
    
    for name, param in self.named_parameters():
        if 'emb_tok' in name:
            embedding_params.append(param)
            print(f"  Embeddingå‚æ•°: {name}, shape: {param.shape}")
        elif 'unembed' in name:
            unembedding_params.append(param)
            print(f"  Unembeddingå‚æ•°: {name}, shape: {param.shape}")
        else:
            matrix_params.append(param)
            print(f"  Matrixå‚æ•°: {name}, shape: {param.shape}")
    
    optimizers = []
    
    # Embeddingä¼˜åŒ–å™¨
    if embedding_params:
        emb_opt = torch.optim.AdamW(
            embedding_params,
            lr=embedding_lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,  # NPUå…¼å®¹æ€§
            fused=False     # ç¦ç”¨fusedä¼˜åŒ–
        )
        optimizers.append(emb_opt)
        print(f"  âœ… Embedding AdamW: lr={embedding_lr}, params={len(embedding_params)}")
    
    # Unembeddingä¼˜åŒ–å™¨
    if unembedding_params:
        unemb_opt = torch.optim.AdamW(
            unembedding_params,
            lr=unembedding_lr,
            weight_decay=0.0,
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,  # NPUå…¼å®¹æ€§
            fused=False     # ç¦ç”¨fusedä¼˜åŒ–
        )
        optimizers.append(unemb_opt)
        print(f"  âœ… Unembedding AdamW: lr={unembedding_lr}, params={len(unembedding_params)}")
    
    # Matrixä¼˜åŒ–å™¨
    if matrix_params:
        matrix_opt = torch.optim.AdamW(
            matrix_params,
            lr=matrix_lr,
            weight_decay=0.0,
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,  # NPUå…¼å®¹æ€§
            fused=False     # ç¦ç”¨fusedä¼˜åŒ–
        )
        optimizers.append(matrix_opt)
        print(f"  âœ… Matrix AdamW: lr={matrix_lr}, params={len(matrix_params)}")
    
    print(f"ðŸŽ¯ æ€»å…±åˆ›å»ºäº† {len(optimizers)} ä¸ªNPUå…¼å®¹çš„AdamWä¼˜åŒ–å™¨")
    return optimizers

# æ›¿æ¢åŽŸå§‹çš„setup_optimizersæ–¹æ³•
GPT.setup_optimizers = npu_compatible_optimizers
print("âœ… NPUå…¼å®¹ä¼˜åŒ–å™¨è¡¥ä¸å·²åº”ç”¨ï¼ˆå®Œå…¨é¿å…Muonï¼‰")
EOF

# 5. è®­ç»ƒtokenizer
echo "4. è®­ç»ƒtokenizer..."
if [ ! -f ~/.cache/nanochat/tokenizer/tokenizer.pkl ]; then
    python -m scripts.tok_train
else
    echo "tokenizerå·²å­˜åœ¨"
fi

# 6. ä¿å®ˆå†…å­˜è®­ç»ƒé…ç½®
echo "5. å¼€å§‹ä¿å®ˆå†…å­˜FineWebè®­ç»ƒ..."
echo ""
echo "ä¿å®ˆé…ç½®:"
echo "  - æ¨¡åž‹æ·±åº¦: 8å±‚ (é™ä½Žå†…å­˜)"
echo "  - æ‰¹æ¬¡å¤§å°: æ¯è®¾å¤‡4, æ€»å…±65536"
echo "  - è®­ç»ƒæ­¥æ•°: 2000"
echo "  - å†…å­˜ä¼˜åŒ–: å¯ç”¨"
echo "  - é¢„è®¡æ—¶é—´: 1-2å°æ—¶"
echo ""

python -c "import npu_adamw_patch" && \
PYTHONPATH=. torchrun --nproc_per_node=4 \
    --master_addr=127.0.0.1 \
    --master_port=29521 \
    scripts/base_train.py \
    --model_tag=fineweb_no_muon_d8 \
    --depth=8 \
    --device_batch_size=4 \
    --total_batch_size=65536 \
    --num_iterations=2000 \
    --embedding_lr=0.01 \
    --unembedding_lr=0.001 \
    --matrix_lr=0.005 \
    --grad_clip=0.8 \
    --eval_every=200 \
    --sample_every=800 \
    --core_metric_every=999999

# 7. æ¸…ç†
rm -f npu_adamw_patch.py

echo ""
echo "ðŸŽ‰ ä¿å®ˆå†…å­˜FineWebè®­ç»ƒå®Œæˆï¼"
echo "æ¨¡åž‹ä¿å­˜: ~/.cache/nanochat/base_checkpoints/fineweb_conservative_d8/"
