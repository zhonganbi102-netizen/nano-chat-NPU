#!/bin/bash

echo "ğŸ”§ ä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨çš„2NPUæµ‹è¯•..."

# ç¯å¢ƒå˜é‡è®¾ç½®
export ASCEND_RT_VISIBLE_DEVICES=0,1
export WORLD_SIZE=2
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500

# HCCLè®¾ç½®
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1

# å†…å­˜è®¾ç½®
export PYTORCH_NPU_ALLOC_CONF="max_split_size_mb:128"

# æ¸…ç†
pkill -f "python.*base_train.py" 2>/dev/null || true
pkill -f "torchrun" 2>/dev/null || true
sleep 2

echo "åˆ›å»ºä¸´æ—¶è®­ç»ƒè„šæœ¬ï¼Œä½¿ç”¨æ ‡å‡†AdamWä¼˜åŒ–å™¨..."

# åˆ›å»ºä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨çš„æµ‹è¯•è„šæœ¬
cat > test_standard_optimizer.py << 'EOF'
import os
import torch
import torch.distributed as dist
import torch_npu

def setup_distributed():
    dist.init_process_group(backend='hccl')
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    torch_npu.npu.set_device(local_rank)
    return rank, world_size, local_rank

def main():
    rank, world_size, local_rank = setup_distributed()
    device = f'npu:{local_rank}'
    
    print(f"Rank {rank}/{world_size} on device {device}")
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    model = torch.nn.Sequential(
        torch.nn.Linear(128, 64),
        torch.nn.ReLU(),
        torch.nn.Linear(64, 32)
    ).to(device)
    
    # ä½¿ç”¨DDP
    model = torch.nn.parallel.DistributedDataParallel(
        model, device_ids=[local_rank], output_device=local_rank
    )
    
    # ä½¿ç”¨æ ‡å‡†AdamW
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
    criterion = torch.nn.MSELoss()
    
    print(f"Rank {rank}: æ¨¡å‹å’Œä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # ç®€å•è®­ç»ƒå¾ªç¯
    for step in range(5):
        # åˆ›å»ºéšæœºæ•°æ®
        inputs = torch.randn(4, 128, device=device)
        targets = torch.randn(4, 32, device=device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        print(f"Rank {rank}: Step {step}, Loss: {loss.item():.4f}")
    
    dist.destroy_process_group()
    print(f"Rank {rank}: è®­ç»ƒå®Œæˆ")

if __name__ == "__main__":
    main()
EOF

echo "å¯åŠ¨æ ‡å‡†ä¼˜åŒ–å™¨2NPUæµ‹è¯•..."

torchrun --standalone --nproc_per_node=2 test_standard_optimizer.py

if [ $? -eq 0 ]; then
    echo "âœ… æ ‡å‡†ä¼˜åŒ–å™¨2NPUæµ‹è¯•æˆåŠŸï¼"
    echo "é—®é¢˜ç¡®å®åœ¨è‡ªå®šä¹‰çš„DistAdamWä¼˜åŒ–å™¨"
else
    echo "âŒ æ ‡å‡†ä¼˜åŒ–å™¨2NPUæµ‹è¯•å¤±è´¥"
fi

# æ¸…ç†æµ‹è¯•æ–‡ä»¶
rm -f test_standard_optimizer.py

echo "æ ‡å‡†ä¼˜åŒ–å™¨æµ‹è¯•å®Œæˆ: $(date)"