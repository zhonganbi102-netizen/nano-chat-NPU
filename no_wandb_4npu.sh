#!/bin/bash

echo "ğŸš€ æ— éœ€wandbçš„4NPUè®­ç»ƒ..."

# ç¯å¢ƒå˜é‡è®¾ç½®
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500

# HCCLè®¾ç½®
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1
export HCCL_CONNECT_TIMEOUT=600

# å†…å­˜è®¾ç½®
export PYTORCH_NPU_ALLOC_CONF="max_split_size_mb:256"

# ç¦ç”¨wandb
export WANDB_MODE=disabled

# æ¸…ç†
pkill -f "python.*base_train.py" 2>/dev/null || true
pkill -f "torchrun" 2>/dev/null || true
sleep 3

echo "å¯åŠ¨æ— wandbçš„4NPUè®­ç»ƒ..."
echo "é…ç½®: ç¦ç”¨wandbï¼Œé¿å…ç™»å½•é—®é¢˜"

torchrun --standalone --nproc_per_node=4 -- scripts/base_train.py \
    --depth=6 \
    --device_batch_size=3 \
    --total_batch_size=49152 \
    --max_seq_len=1024 \
    --num_iterations=20 \
    --eval_every=999999 \
    --core_metric_every=999999 \
    --sample_every=999999 \
    --run="dummy"

echo "è®­ç»ƒå®Œæˆ: $(date)"