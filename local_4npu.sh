#!/bin/bash

echo "ğŸ¯ å•æœº4NPUè®­ç»ƒï¼ˆæœ¬åœ°é€šä¿¡ä¼˜åŒ–ï¼‰..."

# åŸºç¡€ç¯å¢ƒå˜é‡
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3
export WORLD_SIZE=4
export RANK=0
export LOCAL_RANK=0

# ä½¿ç”¨æœ¬åœ°é€šä¿¡ï¼Œé¿å…ç½‘ç»œé—®é¢˜
export MASTER_ADDR=localhost
export MASTER_PORT=29500

# HCCLä¼˜åŒ–è®¾ç½®ï¼ˆå•æœºåœºæ™¯ï¼‰
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1
export HCCL_CONNECT_TIMEOUT=1800  # 30åˆ†é’Ÿè¶…æ—¶
export HCCL_EXEC_TIMEOUT=1800
export HCCL_BUFFSIZE=64

# å†…å­˜å’Œæ€§èƒ½ä¼˜åŒ–
export PYTORCH_NPU_ALLOC_CONF="max_split_size_mb:256"
export OMP_NUM_THREADS=1

# æ¸…ç†ä¹‹å‰çš„è¿›ç¨‹
echo "æ¸…ç†ç¯å¢ƒ..."
pkill -f "python.*base_train.py" 2>/dev/null || true
pkill -f "torchrun" 2>/dev/null || true
sleep 5

# æ¸…ç†NPUç¼“å­˜
python3 -c "
import torch_npu
print('æ¸…ç†NPUç¼“å­˜...')
for i in range(4):
    try:
        torch_npu.npu.set_device(i)
        torch_npu.npu.empty_cache()
        torch_npu.npu.synchronize()
    except:
        pass
print('ç¼“å­˜æ¸…ç†å®Œæˆ')
"

echo ""
echo "é…ç½®ä¿¡æ¯:"
echo "  NPUè®¾å¤‡: 0,1,2,3"
echo "  æ¨¡å‹æ·±åº¦: 6å±‚"
echo "  å•è®¾å¤‡batch: 3"
echo "  æ€»batch: 49152"
echo "  åºåˆ—é•¿åº¦: 1024"
echo "  è®­ç»ƒæ­¥æ•°: 20"

echo ""
echo "å¯åŠ¨å•æœº4NPUè®­ç»ƒ..."

# ä½¿ç”¨è¾ƒå°çš„é…ç½®ï¼Œç¡®ä¿ç¨³å®šæ€§
torchrun \
    --standalone \
    --nproc_per_node=4 \
    --nnodes=1 \
    --node_rank=0 \
    -- \
    scripts/base_train.py \
    --depth=6 \
    --device_batch_size=3 \
    --total_batch_size=49152 \
    --max_seq_len=1024 \
    --num_iterations=20 \
    --eval_every=999999 \
    --core_metric_every=999999 \
    --sample_every=999999 \
    --run="local_4npu_$(date +%Y%m%d_%H%M%S)"

echo ""
echo "è®­ç»ƒå®Œæˆ: $(date)"