#!/bin/bash

# æžç®€å•NPUæµ‹è¯• - è°ƒè¯•å¡æ­»é—®é¢˜
# Minimal single NPU test - debug hanging issues

set -e

echo "ðŸ” æžç®€å•NPUæµ‹è¯• - è°ƒè¯•æ¨¡å¼"
echo ""

# 1. è®¾ç½®çŽ¯å¢ƒ
echo "1. è®¾ç½®çŽ¯å¢ƒ..."
source /usr/local/Ascend/ascend-toolkit/set_env.sh

export ASCEND_RT_VISIBLE_DEVICES=0
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:32
export NPU_COMPILE_DISABLE=1
export TORCH_COMPILE_DISABLE=1
export PYTHONUNBUFFERED=1

echo "âœ… çŽ¯å¢ƒè®¾ç½®å®Œæˆ"

# 2. æ¸…ç†NPU
echo "2. æ¸…ç†NPU..."
python3 -c "
import torch
import torch_npu
if torch_npu.npu.is_available():
    torch_npu.npu.empty_cache()
    print('âœ… NPUæ¸…ç†å®Œæˆ')
"

# 3. åˆ›å»ºæœ€ç®€ä¼˜åŒ–å™¨è¡¥ä¸
echo "3. åˆ›å»ºæœ€ç®€ä¼˜åŒ–å™¨è¡¥ä¸..."
cat > minimal_patch.py << 'EOF'
import torch
import sys
sys.path.insert(0, '.')

print("å¯¼å…¥GPT...")
from nanochat.gpt import GPT

def minimal_optimizer(self, **kwargs):
    """æœ€ç®€ä¼˜åŒ–å™¨å®žçŽ°"""
    print("ðŸ”§ æœ€ç®€AdamWä¼˜åŒ–å™¨")
    
    params = list(self.parameters())
    print(f"å‚æ•°æ•°é‡: {len(params)}")
    
    # æœ€åŸºç¡€çš„AdamW
    optimizer = torch.optim.AdamW(params, lr=0.001)
    print("âœ… ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
    
    return [optimizer]

# æ›¿æ¢æ–¹æ³•
print("åº”ç”¨è¡¥ä¸...")
GPT.setup_optimizers = minimal_optimizer
print("âœ… è¡¥ä¸åº”ç”¨æˆåŠŸ")
EOF

# 4. è¿è¡Œæœ€å°æµ‹è¯•
echo "4. è¿è¡Œæœ€å°æµ‹è¯•..."
python3 -c "
import minimal_patch
print('âœ… è¡¥ä¸åŠ è½½æˆåŠŸ')
"

echo "5. å¼€å§‹æžç®€è®­ç»ƒ..."
timeout 300 python -m scripts.base_train \
    --run=minimal_test \
    --depth=2 \
    --device_batch_size=1 \
    --total_batch_size=2 \
    --num_iterations=5 \
    --embedding_lr=0.001 \
    --unembedding_lr=0.001 \
    --matrix_lr=0.001 \
    --eval_every=999999 \
    --sample_every=999999 \
    --core_metric_every=999999 \
    --verbose

echo ""
if [ $? -eq 0 ]; then
    echo "ðŸŽ‰ æžç®€æµ‹è¯•æˆåŠŸï¼"
else
    echo "âŒ æµ‹è¯•å¤±è´¥æˆ–è¶…æ—¶"
fi

# æ¸…ç†
rm -f minimal_patch.py