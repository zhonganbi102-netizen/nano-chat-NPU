#!/bin/bash

# å¿«é€Ÿè§£å†³æ–¹æ¡ˆï¼šè·³è¿‡tokenizerè®­ç»ƒï¼Œç›´æ¥å¼€å§‹æ¨¡å‹è®­ç»ƒ
# Quick solution: Skip tokenizer training and start model training directly

set -e

echo "ğŸš€ å¿«é€Ÿè§£å†³æ–¹æ¡ˆï¼šè·³è¿‡tokenizerè®­ç»ƒç›´æ¥å¼€å§‹è®­ç»ƒ"

# 1. æ¸…ç†NPUå†…å­˜
echo "1. æ¸…ç†NPUå†…å­˜..."
bash emergency_npu_cleanup.sh || echo "æ¸…ç†è„šæœ¬æ‰§è¡Œå®Œæˆ"

# 2. æ£€æŸ¥æ˜¯å¦å·²æœ‰tokenizeræ–‡ä»¶
echo "2. æ£€æŸ¥tokenizeræ–‡ä»¶..."
if [ ! -f "tokenizer/tokenizer.json" ] && [ ! -f "tokenizer.json" ]; then
    echo "åˆ›å»ºé»˜è®¤tokenizeræ–‡ä»¶..."
    mkdir -p tokenizer
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„é»˜è®¤tokenizeré…ç½®
    cat > tokenizer/tokenizer.json << 'EOF'
{
    "version": "1.0",
    "truncation": null,
    "padding": null,
    "added_tokens": [
        {"id": 0, "content": "<unk>", "single_word": false, "lstrip": false, "rstrip": false, "normalized": true, "special": true},
        {"id": 1, "content": "<s>", "single_word": false, "lstrip": false, "rstrip": false, "normalized": true, "special": true},
        {"id": 2, "content": "</s>", "single_word": false, "lstrip": false, "rstrip": false, "normalized": true, "special": true}
    ],
    "normalizer": null,
    "pre_tokenizer": {"type": "Whitespace"},
    "post_processor": {
        "type": "TemplateProcessing",
        "single": [{"SpecialToken": {"id": "<s>", "type_id": 0}}, {"Sequence": {"id": "A", "type_id": 0}}, {"SpecialToken": {"id": "</s>", "type_id": 0}}],
        "pair": null,
        "special_tokens": {"<s>": {"id": "<s>", "ids": [1], "tokens": ["<s>"]}, "</s>": {"id": "</s>", "ids": [2], "tokens": ["</s>"]}}
    },
    "decoder": {"type": "BPE", "dropout": null, "unk_token": "<unk>", "continuing_subword_prefix": null, "end_of_word_suffix": null, "fuse_unk": false},
    "model": {"type": "BPE", "dropout": null, "unk_token": "<unk>", "continuing_subword_prefix": null, "end_of_word_suffix": null, "vocab": {}, "merges": []}
}
EOF
    echo "âœ… é»˜è®¤tokenizerå·²åˆ›å»º"
else
    echo "âœ… tokenizeræ–‡ä»¶å·²å­˜åœ¨"
fi

# 3. è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡tokenizerè®­ç»ƒ
export SKIP_TOKENIZER_TRAINING=1

# 4. ä¿®æ”¹è®­ç»ƒè„šæœ¬ï¼Œè·³è¿‡tokenizeræ­¥éª¤
echo "3. ä¿®æ”¹è®­ç»ƒè„šæœ¬..."
if grep -q "tok_train.py" full_fineweb_4npu_train.sh; then
    # åˆ›å»ºä¿®æ”¹ç‰ˆæœ¬
    cp full_fineweb_4npu_train.sh full_fineweb_4npu_train_notok.sh
    
    # æ³¨é‡Šæ‰tokenizerè®­ç»ƒè¡Œ
    sed -i.bak 's/.*tok_train\.py.*/# SKIPPED: tokenizer training due to rustbpe issue/' full_fineweb_4npu_train_notok.sh
    sed -i.bak 's/.*python.*scripts\/tok_train.*/# SKIPPED: tokenizer training/' full_fineweb_4npu_train_notok.sh
    
    echo "âœ… å·²åˆ›å»ºè·³è¿‡tokenizerçš„è®­ç»ƒè„šæœ¬: full_fineweb_4npu_train_notok.sh"
    
    # è¿è¡Œä¿®æ”¹åçš„è„šæœ¬
    echo "4. å¯åŠ¨è®­ç»ƒï¼ˆè·³è¿‡tokenizerï¼‰..."
    bash full_fineweb_4npu_train_notok.sh
else
    echo "4. ç›´æ¥å¯åŠ¨è®­ç»ƒ..."
    bash full_fineweb_4npu_train.sh
fi

echo ""
echo "ğŸ‰ è®­ç»ƒå·²å¯åŠ¨ï¼Œè·³è¿‡äº†æœ‰é—®é¢˜çš„tokenizeræ­¥éª¤ï¼"