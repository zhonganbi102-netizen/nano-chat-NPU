"""
NPUå…¼å®¹çš„GPTè®­ç»ƒè„šæœ¬ - ä¸ä½¿ç”¨Muonä¼˜åŒ–å™¨
NPU compatible GPT training script - without Muon optimizer
"""

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import sys
import time
import wandb
import torch

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append('.')

from nanochat.gpt import GPT, GPTConfig
from nanochat.dataloader import tokenizing_distributed_data_loader
from nanochat.common import compute_init, compute_cleanup, print0, DummyWandb, print_banner, get_base_dir
from nanochat.tokenizer import get_tokenizer, get_token_bytes
from nanochat.checkpoint_manager import save_checkpoint
from nanochat.loss_eval import evaluate_bpb
from nanochat.engine import Engine
from scripts.base_eval import evaluate_model

print_banner()

def setup_adamw_only_optimizers(model, unembedding_lr=0.004, embedding_lr=0.2, matrix_lr=0.02, weight_decay=0.0):
    """
    è®¾ç½®åªä½¿ç”¨AdamWçš„ä¼˜åŒ–å™¨ï¼Œé¿å…Muonåœ¨NPUä¸Šçš„å…¼å®¹æ€§é—®é¢˜
    """
    from nanochat.common import get_dist_info
    from nanochat.adamw import DistAdamW
    from functools import partial
    
    model_dim = model.config.n_embd
    ddp, rank, local_rank, world_size = get_dist_info()
    
    # è·å–æ‰€æœ‰å‚æ•°
    matrix_params = list(model.transformer.h.parameters())
    embedding_params = list(model.transformer.wte.parameters())
    lm_head_params = list(model.lm_head.parameters())
    
    # å­¦ä¹ ç‡ç¼©æ”¾
    dmodel_lr_scale = (model_dim / 768) ** -0.5
    if rank == 0:
        print(f"[æ— Muonæ¨¡å¼] ç¼©æ”¾AdamWå­¦ä¹ ç‡ âˆ1/âˆš({model_dim}/768) = {dmodel_lr_scale:.6f}")
    
    # åˆ›å»ºAdamWä¼˜åŒ–å™¨ç»„ï¼ŒåŒ…å«æ‰€æœ‰å‚æ•°
    adam_groups = [
        dict(params=lm_head_params, lr=unembedding_lr * dmodel_lr_scale),
        dict(params=embedding_params, lr=embedding_lr * dmodel_lr_scale),
        dict(params=matrix_params, lr=matrix_lr * dmodel_lr_scale),  # çŸ©é˜µå‚æ•°ä¹Ÿç”¨AdamW
    ]
    
    adamw_kwargs = dict(betas=(0.8, 0.95), eps=1e-10, weight_decay=weight_decay)
    
    # NPUå…¼å®¹çš„AdamW
    try:
        import torch_npu
        if torch_npu.npu.is_available():
            # NPUç¯å¢ƒä½¿ç”¨æ ‡å‡†AdamW
            if ddp:
                AdamWFactory = DistAdamW
            else:
                AdamWFactory = torch.optim.AdamW
            print0("[æ— Muonæ¨¡å¼] ä½¿ç”¨NPUå…¼å®¹çš„AdamWä¼˜åŒ–å™¨")
        else:
            AdamWFactory = partial(torch.optim.AdamW, fused=True)
    except ImportError:
        AdamWFactory = partial(torch.optim.AdamW, fused=True)
    
    adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)
    
    # åªè¿”å›AdamWä¼˜åŒ–å™¨
    optimizers = [adamw_optimizer]
    
    for opt in optimizers:
        for group in opt.param_groups:
            group["initial_lr"] = group["lr"]
    
    print0(f"[æ— Muonæ¨¡å¼] ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆï¼Œå‚æ•°ç»„æ•°: {len(adam_groups)}")
    return optimizers

def main():
    """
    ä¸»è®­ç»ƒå‡½æ•° - å•NPUæ— Muonç‰ˆæœ¬
    """
    # æ£€æŸ¥NPUç¯å¢ƒ
    try:
        import torch_npu
        if torch_npu.npu.is_available():
            torch_npu.npu.set_device(0)
            print(f"âœ… ä½¿ç”¨NPUè®¾å¤‡: npu:{torch_npu.npu.current_device()}")
        else:
            print("âš ï¸  NPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
    except ImportError:
        print("âš ï¸  torch_npuæœªå®‰è£…ï¼Œä½¿ç”¨CPU")
    
    # è®­ç»ƒé…ç½®ï¼ˆä¿å®ˆå‚æ•°ï¼‰
    config = {
        'run': 'single_npu_no_muon',
        'depth': 6,
        'device_batch_size': 4,
        'total_batch_size': 8192,
        'num_iterations': 500,
        'embedding_lr': 0.001,
        'unembedding_lr': 0.0001,
        'matrix_lr': 0.0005,
        'grad_clip': 1.0,
        'eval_every': 100,
        'sample_every': 500,
        'core_metric_every': 999999,
    }
    
    print("ğŸš€ å¼€å§‹å•NPUè®­ç»ƒï¼ˆæ— Muonä¼˜åŒ–å™¨ï¼‰...")
    print(f"é…ç½®: {config}")
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ï¼ˆå•GPUæ¨¡å¼ï¼‰
    compute_init()
    
    # åˆ›å»ºæ¨¡å‹
    model_config = GPTConfig(
        vocab_size=265,  # ç®€åŒ–è¯æ±‡è¡¨
        seq_len=2048,
        depth=config['depth'],
        model_dim=768,
        num_heads=6,
        num_kv_heads=6
    )
    
    model = GPT(model_config)
    print0(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    device = model.get_device()
    print0(f"æ¨¡å‹è®¾å¤‡: {device}")
    
    # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆåªä½¿ç”¨AdamWï¼‰
    optimizers = setup_adamw_only_optimizers(
        model,
        unembedding_lr=config['unembedding_lr'],
        embedding_lr=config['embedding_lr'],
        matrix_lr=config['matrix_lr'],
        weight_decay=0.0
    )
    
    print0("âœ… ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ")
    
    # è·å–tokenizer
    tokenizer = get_tokenizer()
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    loader = tokenizing_distributed_data_loader(
        files="fineweb/*.parquet",
        text_key="text",
        num_workers=1,
        num_epochs=1,
        seed=42,
        verbose=True,
        shuffle=True,
        batch_size=config['device_batch_size'],
        seq_len=model_config.seq_len,
        header_len=1,
        tokenizer=tokenizer,
        rank=0,
        world_size=1
    )
    
    print0("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    
    # è®­ç»ƒå¾ªç¯
    for step in range(config['num_iterations']):
        model.train()
        
        # è·å–æ‰¹æ¬¡æ•°æ®
        batch = next(loader)
        x, y = batch["input_ids"], batch["labels"]
        
        # å‰å‘ä¼ æ’­
        outputs = model(x, targets=y)
        loss = outputs['loss']
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        if config['grad_clip'] > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])
        
        # ä¼˜åŒ–å™¨æ­¥éª¤ï¼ˆåªæœ‰ä¸€ä¸ªAdamWä¼˜åŒ–å™¨ï¼‰
        for opt in optimizers:
            opt.step()
            opt.zero_grad()
        
        # æ‰“å°è¿›åº¦
        if step % 10 == 0:
            print0(f"æ­¥éª¤ {step}/{config['num_iterations']}, æŸå¤±: {loss.item():.4f}")
        
        # è¯„ä¼°
        if step > 0 and step % config['eval_every'] == 0:
            model.eval()
            with torch.no_grad():
                print0(f"è¯„ä¼°æ­¥éª¤ {step}...")
    
    print0("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    compute_cleanup()

if __name__ == "__main__":
    main()