#!/bin/bash

# FineWebæ•°æ®é›†å®Œæ•´è®­ç»ƒè„šæœ¬ - NPU 4å¡ç‰ˆæœ¬
# åŸºäºŽæˆåŠŸçš„4NPUé…ç½®è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒ

set -e

echo "=== FineWebæ•°æ®é›†NPUè®­ç»ƒç®¡é“ ==="

# 0. çŽ¯å¢ƒæ£€æŸ¥å’Œè®¾ç½®
echo "0. çŽ¯å¢ƒæ£€æŸ¥å’Œè®¾ç½®..."

# æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
if [ ! -d "./base_data" ] || [ $(ls ./base_data/shard_*.parquet 2>/dev/null | wc -l) -lt 50 ]; then
    echo "âŒ æ•°æ®é›†ä¸å­˜åœ¨æˆ–æ–‡ä»¶ä¸è¶³ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®ä¸‹è½½è„šæœ¬ï¼š"
    echo "   ./download_fineweb_data.sh"
    exit 1
fi

data_files=$(ls ./base_data/shard_*.parquet 2>/dev/null | wc -l)
echo "âœ… æ£€æµ‹åˆ° $data_files ä¸ªæ•°æ®æ–‡ä»¶"

# è®¾ç½®æ˜‡è…¾çŽ¯å¢ƒ
echo "è®¾ç½®æ˜‡è…¾NPUçŽ¯å¢ƒ..."
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# æ£€æŸ¥NPUçŠ¶æ€
echo "æ£€æŸ¥NPUçŠ¶æ€..."
npu-smi info | head -20

# éªŒè¯torch_npu
python3 -c "
import torch
import torch_npu
assert torch_npu.npu.is_available(), 'NPUä¸å¯ç”¨'
print(f'âœ… NPUè®¾å¤‡æ•°é‡: {torch_npu.npu.device_count()}')
print(f'âœ… torch_npuç‰ˆæœ¬: {torch_npu.__version__}')
"

# 1. æ¸…ç†æ®‹ç•™è¿›ç¨‹
echo "1. æ¸…ç†æ®‹ç•™è¿›ç¨‹..."
if [ -f "./emergency_npu_cleanup.sh" ]; then
    ./emergency_npu_cleanup.sh
    sleep 5
fi

# 2. è®¾ç½®4NPUçŽ¯å¢ƒå˜é‡
echo "2. è®¾ç½®4NPUåˆ†å¸ƒå¼çŽ¯å¢ƒ..."
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29510
export TORCH_COMPILE_DISABLE=1
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:64
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1

echo "NPUé…ç½®:"
echo "  WORLD_SIZE: $WORLD_SIZE"
echo "  MASTER_ADDR: $MASTER_ADDR"
echo "  MASTER_PORT: $MASTER_PORT"

# 3. è®­ç»ƒTokenizer
echo "3. è®­ç»ƒTokenizer..."
echo "ä½¿ç”¨FineWebæ•°æ®è®­ç»ƒtokenizer..."
python -m scripts.tok_train

# 4. åˆ›å»ºä¼˜åŒ–å™¨è¡¥ä¸
echo "4. åˆ›å»º4NPUä¼˜åŒ–å™¨è¡¥ä¸..."
cat > temp_fineweb_patch.py << EOF
import torch
from nanochat.gpt import GPT

def fineweb_optimizers(self, unembedding_lr=0.001, embedding_lr=0.01, matrix_lr=0.01, weight_decay=0.0):
    print("FineWeb 4NPUä¼˜åŒ–å™¨è®¾ç½®: å…¨éƒ¨AdamW")
    embedding_params = [p for n, p in self.named_parameters() if 'emb_tok' in n]
    other_params = [p for n, p in self.named_parameters() if 'emb_tok' not in n]
    opts = []
    if embedding_params:
        opts.append(torch.optim.AdamW([{'params': embedding_params, 'lr': embedding_lr*0.8, 'initial_lr': embedding_lr*0.8}], lr=embedding_lr*0.8, weight_decay=weight_decay, betas=(0.9, 0.95)))
    if other_params:
        opts.append(torch.optim.AdamW([{'params': other_params, 'lr': matrix_lr*0.8, 'initial_lr': matrix_lr*0.8}], lr=matrix_lr*0.8, weight_decay=0.0, betas=(0.9, 0.95)))
    return opts

GPT.setup_optimizers = fineweb_optimizers
print("âœ… FineWeb 4NPUä¼˜åŒ–å™¨è¡¥ä¸å·²åº”ç”¨")
EOF

# 5. Base Modelè®­ç»ƒ - å¤§è§„æ¨¡ç‰ˆæœ¬
echo "5. å¼€å§‹Base Modelè®­ç»ƒ (FineWebæ•°æ®é›†)..."
echo "é…ç½®ï¼š"
echo "  - æ¨¡åž‹æ·±åº¦: 12å±‚"
echo "  - æ‰¹æ¬¡å¤§å°: æ¯è®¾å¤‡16, æ€»æ‰¹æ¬¡262144"
echo "  - è®­ç»ƒæ­¥æ•°: 5000 (å¯æ ¹æ®éœ€è¦è°ƒæ•´)"
echo "  - å­¦ä¹ çŽ‡: ä¼˜åŒ–çš„NPUå…¼å®¹é…ç½®"

python -c "import temp_fineweb_patch" && \
torchrun --nproc_per_node=4 \
    --master_addr=127.0.0.1 \
    --master_port=29510 \
    scripts/base_train.py \
    --run=fineweb_base_d12 \
    --depth=12 \
    --device_batch_size=16 \
    --total_batch_size=262144 \
    --num_iterations=5000 \
    --embedding_lr=0.2 \
    --unembedding_lr=0.004 \
    --matrix_lr=0.02 \
    --grad_clip=1.0 \
    --eval_every=250 \
    --sample_every=1000 \
    --core_metric_every=999999

# 6. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -f temp_fineweb_patch.py

echo ""
echo "ðŸŽ‰ FineWeb Baseè®­ç»ƒå®Œæˆï¼"
echo ""
echo "æ¨¡åž‹ä¿å­˜ä½ç½®: ~/.cache/nanochat/base_checkpoints/fineweb_base_d12/"
echo ""
echo "è®­ç»ƒç»Ÿè®¡ï¼š"
echo "  - æ•°æ®æ–‡ä»¶: $data_files ä¸ª"
echo "  - è®­ç»ƒæ­¥æ•°: 5000"
echo "  - ä½¿ç”¨NPU: 4å¡"
echo ""
echo "ä¸‹ä¸€æ­¥é€‰é¡¹ï¼š"
echo "1. è¿è¡Œä¸­é—´è®­ç»ƒ: ./run_midtraining.sh"
echo "2. è¿›è¡ŒChat SFT: ./run_chat_sft.sh" 
echo "3. æµ‹è¯•æ¨¡åž‹: python -m scripts.chat_cli"
echo "4. å¯åŠ¨WebæœåŠ¡: python -m scripts.chat_web"
