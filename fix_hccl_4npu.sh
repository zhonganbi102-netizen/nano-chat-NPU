#!/bin/bash

echo "ğŸ”§ ä¿®å¤HCCLé€šä¿¡é—®é¢˜çš„4NPUè®­ç»ƒ..."

# è®¾ç½®åŸºç¡€ç¯å¢ƒå˜é‡
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500

# HCCLé€šä¿¡ä¼˜åŒ–è®¾ç½®
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1
export HCCL_CONNECT_TIMEOUT=600  # å¢åŠ è¿æ¥è¶…æ—¶æ—¶é—´åˆ°10åˆ†é’Ÿ
export HCCL_EXEC_TIMEOUT=600     # å¢åŠ æ‰§è¡Œè¶…æ—¶æ—¶é—´
export HCCL_BUFFSIZE=512         # è®¾ç½®ç¼“å†²åŒºå¤§å°

# NPUåŒæ­¥å’Œè°ƒè¯•è®¾ç½®
export ASCEND_LAUNCH_BLOCKING=1   # åŒæ­¥æ¨¡å¼ï¼Œä¾¿äºè°ƒè¯•
export ASCEND_GLOBAL_LOG_LEVEL=1  # è¯¦ç»†æ—¥å¿—
export ASCEND_SLOG_PRINT_TO_STDOUT=1

# å†…å­˜ä¼˜åŒ–è®¾ç½®
export PYTORCH_NPU_ALLOC_CONF="max_split_size_mb:512"

# æ¸…ç†è¿›ç¨‹å’Œå†…å­˜
echo "æ¸…ç†ä¹‹å‰çš„è¿›ç¨‹..."
pkill -f "python.*base_train.py" 2>/dev/null || true
pkill -f "torchrun" 2>/dev/null || true
sleep 5

# æ¸…ç†NPUå†…å­˜å’ŒçŠ¶æ€
echo "é‡ç½®NPUçŠ¶æ€..."
python3 -c "
import torch_npu
import time

print('æ¸…ç†NPUå†…å­˜...')
for i in range(4):
    try:
        torch_npu.npu.set_device(i)
        torch_npu.npu.empty_cache()
        torch_npu.npu.synchronize()
        print(f'NPU {i} æ¸…ç†å®Œæˆ')
    except Exception as e:
        print(f'NPU {i} æ¸…ç†å¤±è´¥: {e}')

time.sleep(2)
print('NPUçŠ¶æ€é‡ç½®å®Œæˆ')
"

# æ£€æŸ¥NPUé€šä¿¡èƒ½åŠ›
echo "æ£€æŸ¥NPUè®¾å¤‡çŠ¶æ€..."
python3 -c "
import torch_npu
import torch
import torch.distributed as dist

try:
    device_count = torch_npu.npu.device_count()
    print(f'å¯ç”¨NPUæ•°é‡: {device_count}')
    
    for i in range(min(4, device_count)):
        torch_npu.npu.set_device(i)
        print(f'NPU {i}: {torch_npu.npu.get_device_name(i)}')
        
        # æµ‹è¯•å†…å­˜åˆ†é…
        x = torch.randn(100, 100, device=f'npu:{i}')
        print(f'NPU {i} å†…å­˜æµ‹è¯•é€šè¿‡')
        del x
        torch_npu.npu.empty_cache()
        
except Exception as e:
    print(f'NPUæ£€æŸ¥å¤±è´¥: {e}')
"

echo ""
echo "å¯åŠ¨HCCLä¼˜åŒ–çš„4NPUè®­ç»ƒ..."
echo "é…ç½®: åŒæ­¥æ¨¡å¼ï¼Œå»¶é•¿è¶…æ—¶ï¼Œä¼˜åŒ–é€šä¿¡"

# å¯åŠ¨è®­ç»ƒï¼Œè·³è¿‡åˆå§‹è¯„ä¼°
torchrun --standalone --nproc_per_node=4 -- scripts/base_train.py \
    --depth=8 \
    --device_batch_size=4 \
    --total_batch_size=131072 \
    --max_seq_len=1024 \
    --num_iterations=50 \
    --eval_every=999999 \
    --core_metric_every=999999 \
    --sample_every=999999 \
    --run="4npu_hccl_fix_$(date +%Y%m%d_%H%M%S)"

echo ""
echo "è®­ç»ƒå®Œæˆ: $(date)"