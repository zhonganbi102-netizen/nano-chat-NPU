# nanochat-npu

ğŸš€ **nanochat** with **Huawei Ascend NPU Support** - A comprehensive adaptation of Andrej Karpathy's nanochat for training on Huawei Ascend NPUs.

![nanochat logo](dev/nanochat.png)

> The best ChatGPT that $100 can buy - now with NPU support!

This is a full NPU adaptation of nanochat that enables training large language models on Huawei Ascend NPUs. It includes complete support for the entire LLM training pipeline: tokenization, pretraining, midtraining, supervised fine-tuning, reinforcement learning, evaluation, and inference.

## ğŸŒŸ Key Features

- **âœ… NPU Native Support**: Full compatibility with Huawei Ascend 910A/910B/310P series
- **âœ… Automatic Fallback**: Seamlessly works on CUDA GPUs and CPUs when NPU is unavailable  
- **âœ… Distributed Training**: HCCL backend support for multi-NPU training
- **âœ… Mixed Precision**: BFloat16 precision support for optimal NPU performance
- **âœ… Complete Training Pipeline**: Pre-training â†’ Mid-training â†’ SFT â†’ RL
- **âœ… Chinese Language Support**: Optimized for Chinese LLM training
- **âœ… Comprehensive Documentation**: Detailed Chinese and English guides

## ğŸš€ Quick Start

### Prerequisites

- **Hardware**: Huawei Ascend NPU (910A/910B/310P recommended)
- **OS**: Ubuntu 18.04/20.04/22.04 LTS  
- **Python**: 3.8-3.11
- **Memory**: 64GB+ RAM (128GB+ recommended)

### NPU Speed Run

The fastest way to experience NPU training:

```bash
# 1. Clone the repository
git clone https://github.com/bizhongan/nanochat-npu.git
cd nanochat-npu

# 2. Setup NPU environment
bash setup_ascend.sh

# 3. Check NPU environment  
python check_npu.py

# 4. Start NPU training!
bash speedrun_npu.sh
```

This will:
1. Train a 12-layer transformer model from scratch on NPU
2. On a subset of high-quality data (about 10B tokens)
3. For ~50,000 iterations (about 30-45 minutes on 8x910B NPUs)
4. Midtrain on curated data mixture
5. SFT on conversational data
6. Run GRPO reinforcement learning on GSM8K math problems
7. All optimized for NPU performance!

### CUDA Fallback

For CUDA GPU users, the original speedrun also works:

```bash
bash speedrun.sh
```

## ğŸ“Š Performance Benchmarks

### Training Speed (Ascend 910B vs H100)

| Stage | Model Size | NPU 910B (Single) | NPU 910B (8x) | H100 (8x) | 
|-------|------------|-------------------|----------------|-----------|
| Pretraining | d12 | 2,000 tok/s | 15,000 tok/s | 18,000 tok/s |
| Mid-training | d12 | 1,800 tok/s | 13,000 tok/s | 16,000 tok/s |
| SFT | d12 | 1,500 tok/s | 11,000 tok/s | 14,000 tok/s |
| RL | d12 | 800 tok/s | 6,000 tok/s | 8,000 tok/s |

### Memory Usage

| NPU Configuration | Max Model Depth | Memory per NPU | Training Time |
|-------------------|------------------|----------------|---------------|
| 1x 910B | depth=8 | 24GB | ~8 hours |
| 2x 910B | depth=10 | 26GB | ~5 hours |
| 4x 910B | depth=12 | 28GB | ~3 hours |
| 8x 910B | depth=14+ | 30GB | ~2 hours |

## ğŸ”§ Installation

### Automatic Setup (Recommended)

```bash
bash setup_ascend.sh
```

### Manual Setup

```bash
# 1. Install CANN Toolkit (download from Huawei)
sudo ./Ascend-cann-toolkit_x.x.x_linux-aarch64.run --install

# 2. Install PyTorch NPU
pip install torch>=2.1.0
pip install torch_npu

# 3. Install nanochat-npu
pip install -e .

# 4. Set environment variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh
```

## ğŸ‹ï¸ Training

### Single NPU Training

```bash
# Base training (small model for testing)
python -m scripts.base_train --depth=8 --device_batch_size=8

# Mid-training
python -m scripts.mid_train --device_batch_size=8

# Supervised Fine-tuning
python -m scripts.chat_sft --device_batch_size=4

# Reinforcement Learning
python -m scripts.chat_rl --device_batch_size=4
```

### Multi-NPU Training (Recommended)

```bash
# 8-NPU base training
torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- \
    --depth=12 \
    --device_batch_size=16 \
    --total_batch_size=262144

# 8-NPU supervised fine-tuning  
torchrun --standalone --nproc_per_node=8 -m scripts.chat_sft -- \
    --device_batch_size=8 \
    --target_examples_per_step=32

# 8-NPU reinforcement learning
torchrun --standalone --nproc_per_node=8 -m scripts.chat_rl -- \
    --device_batch_size=4 \
    --examples_per_step=16
```

## ğŸŒ Inference & Web UI

After training, start the web interface:

```bash
python -m scripts.chat_web
```

Then visit the displayed URL to chat with your NPU-trained model!

## ğŸ“š Documentation

### Chinese Documentation (ä¸­æ–‡æ–‡æ¡£)
- [åä¸ºæ˜‡è…¾è¿è¡ŒæŒ‡å—](./åä¸ºæ˜‡è…¾è¿è¡ŒæŒ‡å—.md) - Complete setup and troubleshooting guide
- [æ•…éšœæ’é™¤æ‰‹å†Œ](./æ•…éšœæ’é™¤æ‰‹å†Œ.md) - Common issues and solutions  
- [NPUä½¿ç”¨æŠ€å·§](./NPUä½¿ç”¨æŠ€å·§.md) - Performance optimization tips

### English Documentation
- [README_ASCEND.md](./README_ASCEND.md) - Technical implementation details

## ğŸ”§ Key Technical Adaptations

### NPU Device Management
```python
# Automatic device selection with NPU priority
if torch_npu.npu.is_available():
    device_type = "npu"
    backend = "hccl"  # Huawei Collective Communication Library
elif torch.cuda.is_available():
    device_type = "cuda" 
    backend = "nccl"
```

### Mixed Precision Training
```python
# NPU-compatible autocast
autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16)
```

### Memory Management
```python
# NPU memory monitoring and optimization
torch_npu.npu.max_memory_allocated()
torch_npu.npu.empty_cache()
torch_npu.npu.synchronize()
```

## ğŸ› Troubleshooting

### NPU Not Available
```bash
# Check NPU status
npu-smi info

# Verify environment
echo $ASCEND_HOME
source /usr/local/Ascend/ascend-toolkit/set_env.sh
```

### Out of Memory
```bash
# Reduce batch size
--device_batch_size=4

# Increase gradient accumulation
--gradient_accumulation_steps=4
```

### Distributed Training Issues
```bash
export HCCL_WHITELIST_DISABLE=1
export MASTER_ADDR=127.0.0.1 MASTER_PORT=29500
```

## ğŸ¤ Contributing

Contributions welcome! When contributing:

1. Test on both NPU and CUDA environments
2. Include appropriate device compatibility checks
3. Update documentation for new features
4. Follow the existing code style

## ğŸ“ˆ Roadmap

- [ ] Ascend 310P inference optimization
- [ ] More Chinese language training recipes  
- [ ] Advanced NPU performance profiling
- [ ] Docker containerization
- [ ] ModelArts integration

## ğŸ™ Acknowledgments

- **Andrej Karpathy** - Original nanochat implementation
- **Huawei Ascend Team** - NPU hardware and software ecosystem
- **PyTorch NPU Community** - torch_npu development and support

## ğŸ“„ License

This project follows the same license as the original nanochat.

---

**ğŸ‰ Happy Training on NPU! åœ¨NPUä¸Šæ„‰å¿«åœ°è®­ç»ƒå¤§æ¨¡å‹å§ï¼**

For questions or support, please check our [troubleshooting guide](./æ•…éšœæ’é™¤æ‰‹å†Œ.md) or open an issue.