#!/bin/bash

echo "ğŸ’¾ 4NPUå†…å­˜ä¼˜åŒ–è®­ç»ƒ..."

# è®¾ç½®ç¯å¢ƒå˜é‡
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1

# NPUå†…å­˜ä¼˜åŒ–è®¾ç½®
export PYTORCH_NPU_ALLOC_CONF="max_split_size_mb:512"

# æ¸…ç†è¿›ç¨‹å’Œå†…å­˜
pkill -f "python.*base_train.py" 2>/dev/null || true
pkill -f "torchrun" 2>/dev/null || true
sleep 3

# æ¸…ç†NPUå†…å­˜
python3 -c "
import torch_npu
for i in range(4):
    try:
        torch_npu.npu.set_device(i)
        torch_npu.npu.empty_cache()
        print(f'æ¸…ç†NPU {i} å†…å­˜')
    except:
        pass
"

echo "å¯åŠ¨å†…å­˜ä¼˜åŒ–çš„4NPUè®­ç»ƒ..."
echo "é…ç½®: 4ä¸ªNPU, depth=8 (å‡å°æ¨¡å‹), device_batch_size=4 (å‡å°batch), æ¢¯åº¦ç´¯ç§¯"

# å†…å­˜ä¼˜åŒ–é…ç½®ï¼šæ›´å°çš„æ¨¡å‹å’Œbatch size
torchrun --standalone --nproc_per_node=4 -- scripts/base_train.py \
    --depth=8 \
    --device_batch_size=4 \
    --total_batch_size=131072 \
    --max_seq_len=1024 \
    --num_iterations=100 \
    --run="4npu_memory_opt_$(date +%Y%m%d_%H%M%S)"

echo "è®­ç»ƒå®Œæˆ: $(date)"