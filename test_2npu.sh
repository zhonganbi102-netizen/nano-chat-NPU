#!/bin/bash

echo "ğŸ§ª 2NPUæµ‹è¯•è®­ç»ƒ..."

# ä½¿ç”¨2ä¸ªNPUè¿›è¡Œæ›´ç®€å•çš„æµ‹è¯•
export ASCEND_RT_VISIBLE_DEVICES=0,1
export WORLD_SIZE=2
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500

# HCCLè®¾ç½®
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1

# å†…å­˜è®¾ç½®
export PYTORCH_NPU_ALLOC_CONF="max_split_size_mb:256"

# æ¸…ç†
pkill -f "python.*base_train.py" 2>/dev/null || true
pkill -f "torchrun" 2>/dev/null || true
sleep 3

# æ¸…ç†NPUç¼“å­˜
python3 -c "
import torch_npu
for i in range(2):
    try:
        torch_npu.npu.set_device(i)
        torch_npu.npu.empty_cache()
        torch_npu.npu.synchronize()
    except:
        pass
"

echo "å¯åŠ¨2NPUæµ‹è¯•..."
echo "é…ç½®: depth=4, batch=2, ä»…10æ­¥è®­ç»ƒ"

torchrun --standalone --nproc_per_node=2 -- scripts/base_train.py \
    --depth=4 \
    --device_batch_size=2 \
    --total_batch_size=16384 \
    --max_seq_len=512 \
    --num_iterations=10 \
    --eval_every=999999 \
    --core_metric_every=999999 \
    --sample_every=999999 \
    --run="2npu_test_$(date +%Y%m%d_%H%M%S)"

if [ $? -eq 0 ]; then
    echo "âœ… 2NPUæµ‹è¯•æˆåŠŸï¼å¯ä»¥å°è¯•4NPUè®­ç»ƒ"
else
    echo "âŒ 2NPUæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•"
fi

echo "æµ‹è¯•å®Œæˆ: $(date)"