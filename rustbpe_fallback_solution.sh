#!/bin/bash

# å¿«é€Ÿå¤‡ç”¨è§£å†³æ–¹æ¡ˆ - ä½¿ç”¨HuggingFace tokenizeræ›¿ä»£rustbpe
# Quick fallback solution - Use HuggingFace tokenizer instead of rustbpe

set -e

echo "=== RustBPEå¤‡ç”¨è§£å†³æ–¹æ¡ˆ ==="
echo "RustBPE Fallback Solution"

# 1. å®‰è£…HuggingFace tokenizersä½œä¸ºå¤‡ç”¨
echo "1. å®‰è£…HuggingFace tokenizers..."
pip install tokenizers

# 2. åˆ›å»ºå¤‡ç”¨tokenizerè®­ç»ƒè„šæœ¬
echo "2. åˆ›å»ºå¤‡ç”¨tokenizerè„šæœ¬..."
cat > scripts/tok_train_fallback.py << 'EOF'
"""
å¤‡ç”¨tokenizerè®­ç»ƒè„šæœ¬ - ä½¿ç”¨HuggingFace tokenizers
Fallback tokenizer training script using HuggingFace tokenizers
"""

import os
import sys
import argparse
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.processors import TemplateProcessing

def train_tokenizer_fallback(vocab_size=65536):
    """ä½¿ç”¨HuggingFace tokenizersè®­ç»ƒBPE tokenizer"""
    print(f"ğŸ”„ ä½¿ç”¨HuggingFace tokenizersè®­ç»ƒå¤‡ç”¨tokenizer (vocab_size={vocab_size})")
    
    # åˆ›å»ºBPE tokenizer
    tokenizer = Tokenizer(BPE(unk_token="<unk>"))
    tokenizer.pre_tokenizer = Whitespace()
    
    # è®¾ç½®ç‰¹æ®Štoken
    special_tokens = ["<unk>", "<s>", "</s>"]
    
    # åˆ›å»ºtrainer
    trainer = BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        min_frequency=2
    )
    
    # ä»æ•°æ®æ–‡ä»¶è®­ç»ƒ
    from nanochat.common import get_base_dir
    base_dir = get_base_dir()
    data_dir = os.path.join(base_dir, "fineweb")
    
    # æŸ¥æ‰¾parquetæ–‡ä»¶
    import glob
    parquet_files = glob.glob(os.path.join(data_dir, "*.parquet"))
    if not parquet_files:
        raise FileNotFoundError(f"æ²¡æœ‰åœ¨ {data_dir} ä¸­æ‰¾åˆ°parquetæ–‡ä»¶")
    
    print(f"æ‰¾åˆ° {len(parquet_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    
    # åˆ›å»ºæ–‡æœ¬è¿­ä»£å™¨
    def text_iterator():
        import pandas as pd
        count = 0
        for file in parquet_files[:5]:  # åªä½¿ç”¨å‰5ä¸ªæ–‡ä»¶å¿«é€Ÿè®­ç»ƒ
            print(f"å¤„ç†æ–‡ä»¶: {file}")
            df = pd.read_parquet(file)
            for text in df['text']:
                yield text
                count += 1
                if count >= 100000:  # é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡
                    return
    
    # è®­ç»ƒtokenizer
    print("å¼€å§‹è®­ç»ƒtokenizer...")
    tokenizer.train_from_iterator(text_iterator(), trainer)
    
    # è®¾ç½®åå¤„ç†å™¨
    tokenizer.post_processor = TemplateProcessing(
        single="<s> $A </s>",
        special_tokens=[
            ("<s>", tokenizer.token_to_id("<s>")),
            ("</s>", tokenizer.token_to_id("</s>")),
        ]
    )
    
    # ä¿å­˜tokenizer
    tokenizer_dir = os.path.join(base_dir, "tokenizer")
    os.makedirs(tokenizer_dir, exist_ok=True)
    tokenizer_path = os.path.join(tokenizer_dir, "tokenizer.json")
    tokenizer.save(tokenizer_path)
    
    print(f"âœ… å¤‡ç”¨tokenizerè®­ç»ƒå®Œæˆï¼Œä¿å­˜åˆ°: {tokenizer_path}")
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}")
    
    return tokenizer

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--vocab_size", type=int, default=65536)
    args = parser.parse_args()
    
    train_tokenizer_fallback(args.vocab_size)
EOF

# 3. ä¿®æ”¹tokenizer.pyä»¥æ”¯æŒå¤‡ç”¨æ–¹æ¡ˆ
echo "3. åˆ›å»ºtokenizerå¤‡ç”¨è¡¥ä¸..."
cat > tokenizer_fallback_patch.py << 'EOF'
"""
ä¸ºtokenizer.pyæ·»åŠ å¤‡ç”¨æ”¯æŒçš„è¡¥ä¸
"""

# åœ¨nanochat/tokenizer.pyä¸­æ·»åŠ å¤‡ç”¨import
fallback_import = '''
# å¤‡ç”¨tokenizeræ”¯æŒ
try:
    import rustbpe
    RUSTBPE_AVAILABLE = True
except ImportError:
    RUSTBPE_AVAILABLE = False
    try:
        from tokenizers import Tokenizer
        HUGGINGFACE_TOKENIZERS_AVAILABLE = True
    except ImportError:
        HUGGINGFACE_TOKENIZERS_AVAILABLE = False
'''

print("æ·»åŠ ä»¥ä¸‹ä»£ç åˆ°nanochat/tokenizer.pyçš„é¡¶éƒ¨:")
print(fallback_import)
EOF

echo "âœ… å¤‡ç”¨è§£å†³æ–¹æ¡ˆå‡†å¤‡å®Œæˆ"
echo ""
echo "ç°åœ¨è¯·é€‰æ‹©ä»¥ä¸‹æ–¹æ¡ˆä¹‹ä¸€:"
echo "æ–¹æ¡ˆ1: å°è¯•ä¿®å¤rustbpe"
echo "  ./fix_rustbpe_server.sh"
echo ""
echo "æ–¹æ¡ˆ2: ä½¿ç”¨å¤‡ç”¨tokenizer"
echo "  python scripts/tok_train_fallback.py"
echo ""
echo "æ–¹æ¡ˆ3: è·³è¿‡tokenizerè®­ç»ƒï¼ˆä½¿ç”¨é¢„è®­ç»ƒçš„ï¼‰"
echo "  # ç›´æ¥è¿è¡Œè®­ç»ƒï¼Œä¼šä½¿ç”¨é»˜è®¤tokenizer"