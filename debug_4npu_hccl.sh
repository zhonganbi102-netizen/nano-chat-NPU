#!/bin/bash

echo "ðŸ” è°ƒè¯•4NPU HCCLé€šä¿¡é—®é¢˜..."

# 1. æ£€æŸ¥NPUè®¾å¤‡çŠ¶æ€
echo "ðŸ“Š æ£€æŸ¥NPUè®¾å¤‡çŠ¶æ€:"
npu-smi info

# 2. æ£€æŸ¥HCCLé€šä¿¡çŽ¯å¢ƒ
echo "ðŸ”— æ£€æŸ¥HCCLé€šä¿¡çŽ¯å¢ƒ:"
export HCCL_CONNECT_TIMEOUT=300  # å¢žåŠ è¶…æ—¶æ—¶é—´åˆ°5åˆ†é’Ÿ
export HCCL_EXEC_TIMEOUT=300
export ASCEND_LAUNCH_BLOCKING=1  # åŒæ­¥æ¨¡å¼ï¼Œä¾¿äºŽè°ƒè¯•
export PYTHONFAULTHANDLER=1

# 3. æµ‹è¯•åŸºç¡€HCCLé€šä¿¡
echo "ðŸ§ª æµ‹è¯•åŸºç¡€HCCLé€šä¿¡..."
cat > test_hccl_simple.py << 'EOF'
import os
import torch
import torch_npu
import torch.distributed as dist

def init_process():
    try:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
        
        print(f"[Rank {rank}] åˆå§‹åŒ–è¿›ç¨‹ç»„...")
        
        # è®¾ç½®è®¾å¤‡
        torch_npu.npu.set_device(local_rank)
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        dist.init_process_group(
            backend='hccl',
            rank=rank,
            world_size=world_size,
            timeout=torch.distributed.utils.get_default_timeout() * 3  # 3å€è¶…æ—¶
        )
        
        print(f"[Rank {rank}] è¿›ç¨‹ç»„åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•ç®€å•çš„all_reduce
        device = f'npu:{local_rank}'
        tensor = torch.ones(10, device=device) * rank
        print(f"[Rank {rank}] å‘é€å¼ é‡: {tensor[:5]}")
        
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
        print(f"[Rank {rank}] æŽ¥æ”¶å¼ é‡: {tensor[:5]}")
        
        # æ¸…ç†
        dist.destroy_process_group()
        print(f"[Rank {rank}] æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"[Rank {rank}] é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    init_process()
EOF

# 4. è¿è¡ŒHCCLé€šä¿¡æµ‹è¯•
echo "ðŸš€ è¿è¡ŒHCCLé€šä¿¡æµ‹è¯•..."
torchrun \
    --nproc_per_node=4 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=127.0.0.1 \
    --master_port=12355 \
    test_hccl_simple.py

echo "âœ… HCCLé€šä¿¡æµ‹è¯•å®Œæˆ"