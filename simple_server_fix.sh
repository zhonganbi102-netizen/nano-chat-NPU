#!/bin/bash

# åä¸ºæœåŠ¡å™¨æœ€ç®€è§£å†³æ–¹æ¡ˆ - ç›´æ¥è·³è¿‡æœ‰é—®é¢˜çš„éƒ¨åˆ†
# Simplest solution for Huawei server - skip problematic parts

echo "ğŸš€ åä¸ºæœåŠ¡å™¨æœ€ç®€è§£å†³æ–¹æ¡ˆ"
echo "è·³è¿‡å¤æ‚çš„rustbpeç¼–è¯‘ï¼Œç›´æ¥å¼€å§‹è®­ç»ƒ"

# 1. æ¸…ç†NPU
echo "1. æ¸…ç†NPUå†…å­˜..."
source /usr/local/Ascend/ascend-toolkit/set_env.sh
python3 -c "
import torch
import torch_npu
if torch_npu.npu.is_available():
    for i in range(torch_npu.npu.device_count()):
        torch_npu.npu.set_device(i)
        torch_npu.npu.empty_cache()
    print(f'âœ… æ¸…ç†äº† {torch_npu.npu.device_count()} ä¸ªNPUè®¾å¤‡')
else:
    print('âš ï¸  NPUä¸å¯ç”¨')
"

# 2. å®‰è£…å¿…è¦ä¾èµ–ï¼ˆè·³è¿‡rustbpeï¼‰
echo "2. å®‰è£…å¿…è¦ä¾èµ–..."
pip install datasets fastapi files-to-prompt numpy==1.26.4 psutil regex tiktoken tokenizers uvicorn wandb --root-user-action=ignore

# 3. åˆ›å»ºé»˜è®¤tokenizer
echo "3. åˆ›å»ºé»˜è®¤tokenizer..."
mkdir -p tokenizer

# ä½¿ç”¨Pythonåˆ›å»ºä¸€ä¸ªç®€å•çš„tokenizer
python3 -c "
import json
import os

# åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„tokenizeré…ç½®
tokenizer_config = {
    'version': '1.0',
    'truncation': None,
    'padding': None,
    'added_tokens': [
        {'id': 0, 'content': '<unk>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True, 'special': True},
        {'id': 1, 'content': '<s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True, 'special': True},
        {'id': 2, 'content': '</s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True, 'special': True}
    ],
    'normalizer': None,
    'pre_tokenizer': {'type': 'Whitespace'},
    'post_processor': {
        'type': 'TemplateProcessing',
        'single': [
            {'SpecialToken': {'id': '<s>', 'type_id': 0}}, 
            {'Sequence': {'id': 'A', 'type_id': 0}}, 
            {'SpecialToken': {'id': '</s>', 'type_id': 0}}
        ],
        'pair': None,
        'special_tokens': {
            '<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}, 
            '</s>': {'id': '</s>', 'ids': [2], 'tokens': ['</s>']}
        }
    },
    'decoder': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False},
    'model': {
        'type': 'BPE', 
        'dropout': None, 
        'unk_token': '<unk>', 
        'continuing_subword_prefix': None, 
        'end_of_word_suffix': None, 
        'vocab': {'<unk>': 0, '<s>': 1, '</s>': 2}, 
        'merges': []
    }
}

with open('tokenizer/tokenizer.json', 'w') as f:
    json.dump(tokenizer_config, f, indent=2)

print('âœ… é»˜è®¤tokenizerå·²åˆ›å»º')
"

# 4. ä¿®æ”¹è®­ç»ƒè„šæœ¬ï¼Œè·³è¿‡tokenizerè®­ç»ƒ
echo "4. ä¿®æ”¹è®­ç»ƒè„šæœ¬..."
if [ -f "full_fineweb_4npu_train.sh" ]; then
    # åˆ›å»ºä¿®æ”¹ç‰ˆæœ¬
    cp full_fineweb_4npu_train.sh full_fineweb_4npu_train_fixed.sh
    
    # æ³¨é‡Šæ‰æ‰€æœ‰tokenizerç›¸å…³çš„è¡Œ
    sed -i 's/.*tok_train\.py.*/echo "SKIPPED: tokenizer training"/' full_fineweb_4npu_train_fixed.sh
    sed -i 's/.*python.*scripts\/tok_train.*/echo "SKIPPED: tokenizer training"/' full_fineweb_4npu_train_fixed.sh
    
    echo "âœ… è®­ç»ƒè„šæœ¬å·²ä¿®æ”¹: full_fineweb_4npu_train_fixed.sh"
else
    echo "âŒ æ²¡æœ‰æ‰¾åˆ°full_fineweb_4npu_train.sh"
    echo "è¯·ç¡®è®¤æ–‡ä»¶å­˜åœ¨"
    exit 1
fi

# 5. æ£€æŸ¥æ•°æ®
echo "5. æ£€æŸ¥FineWebæ•°æ®..."
data_files=$(find . -name "*.parquet" 2>/dev/null | wc -l)
echo "æ‰¾åˆ° $data_files ä¸ªparquetæ–‡ä»¶"

if [ "$data_files" -lt 10 ]; then
    echo "âš ï¸  æ•°æ®æ–‡ä»¶è¾ƒå°‘ï¼Œä½†ä»å¯è®­ç»ƒ"
fi

# 6. å¯åŠ¨è®­ç»ƒ
echo "6. å¯åŠ¨è®­ç»ƒ..."
chmod +x full_fineweb_4npu_train_fixed.sh emergency_npu_cleanup.sh

echo ""
echo "ğŸ‰ å‡†å¤‡å®Œæˆï¼ç°åœ¨å¯åŠ¨è®­ç»ƒï¼š"
echo "bash full_fineweb_4npu_train_fixed.sh"
echo ""
echo "å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥æ‰§è¡Œï¼š"
echo "bash emergency_npu_cleanup.sh  # æ¸…ç†NPU"
echo ""

# è‡ªåŠ¨å¯åŠ¨è®­ç»ƒ
bash full_fineweb_4npu_train_fixed.sh