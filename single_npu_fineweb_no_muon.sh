#!/bin/bash

# å•NPU FineWebè®­ç»ƒ - ä¸ä½¿ç”¨Muonä¼˜åŒ–å™¨
# Single NPU FineWeb training - without Muon optimizer

set -e

echo "ğŸš€ å•NPU FineWebè®­ç»ƒ - AdamWä¼˜åŒ–å™¨ç‰ˆæœ¬"
echo "=================================================="

# 1. ç¯å¢ƒè®¾ç½®
echo "1. è®¾ç½®NPUç¯å¢ƒ..."
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# å•NPUç¯å¢ƒå˜é‡
export ASCEND_RT_VISIBLE_DEVICES=0
export WORLD_SIZE=1
export RANK=0
export LOCAL_RANK=0
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29800

# NPUå†…å­˜ä¼˜åŒ–
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:128
export NPU_COMPILE_DISABLE=1
export TORCH_NPU_DISABLE_LAZY_INIT=1

# Pythonä¼˜åŒ–
export PYTHONUNBUFFERED=1
export OMP_NUM_THREADS=1

echo "âœ… ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ"
echo "  è®¾å¤‡: NPU:0"
echo "  å†…å­˜ä¼˜åŒ–: 128MBåˆ†å‰²"
echo "  ç¼–è¯‘ä¼˜åŒ–: ç¦ç”¨"

# 2. æ¸…ç†NPUå†…å­˜
echo ""
echo "2. æ¸…ç†NPUå†…å­˜..."
python3 -c "
import torch
import torch_npu
import gc
import time

if torch_npu.npu.is_available():
    print(f'æ¸…ç† {torch_npu.npu.device_count()} ä¸ªNPUè®¾å¤‡...')
    for i in range(torch_npu.npu.device_count()):
        torch_npu.npu.set_device(i)
        torch_npu.npu.empty_cache()
        torch_npu.npu.synchronize()
    
    time.sleep(1)
    gc.collect()
    print('âœ… NPUå†…å­˜æ¸…ç†å®Œæˆ')
"

# 3. æ£€æŸ¥æ•°æ®
echo ""
echo "3. æ£€æŸ¥FineWebæ•°æ®..."
data_files=$(find . -name "*.parquet" 2>/dev/null | wc -l)
echo "æ‰¾åˆ° $data_files ä¸ªparquetæ–‡ä»¶"

if [ "$data_files" -lt 5 ]; then
    echo "âš ï¸  æ•°æ®æ–‡ä»¶è¾ƒå°‘ï¼Œä½†ç»§ç»­è®­ç»ƒ..."
fi

# 4. æ£€æŸ¥tokenizer
echo ""
echo "4. æ£€æŸ¥tokenizer..."
if [ ! -f "tokenizer/tokenizer.json" ]; then
    echo "åˆ›å»ºç®€å•tokenizer..."
    mkdir -p tokenizer
    python3 -c "
import json
tokenizer_config = {
    'version': '1.0',
    'model': {'type': 'BPE', 'vocab': {'<unk>': 0, '<s>': 1, '</s>': 2}, 'merges': []},
    'pre_tokenizer': {'type': 'Whitespace'},
    'post_processor': {'type': 'TemplateProcessing'}
}
with open('tokenizer/tokenizer.json', 'w') as f:
    json.dump(tokenizer_config, f)
print('âœ… ç®€å•tokenizerå·²åˆ›å»º')
"
else
    echo "âœ… tokenizerå·²å­˜åœ¨"
fi

# 5. å¼€å§‹å•NPUè®­ç»ƒï¼ˆä¸ä½¿ç”¨Muonï¼‰
echo ""
echo "5. å¼€å§‹å•NPU FineWebè®­ç»ƒ..."
echo "âš ï¸  ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œé¿å…Muonå…¼å®¹æ€§é—®é¢˜"
echo ""

# åˆ›å»ºè®­ç»ƒè„šæœ¬
cat > temp_single_npu_train.py << 'EOF'
import os
import sys
import torch
import torch_npu

# è®¾ç½®NPUè®¾å¤‡
if torch_npu.npu.is_available():
    torch_npu.npu.set_device(0)
    print(f"ä½¿ç”¨è®¾å¤‡: npu:{torch_npu.npu.current_device()}")

# å¯¼å…¥è®­ç»ƒæ¨¡å—
sys.path.append('.')

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨å•NPU FineWebè®­ç»ƒï¼ˆæ— Muonï¼‰...")
    
    # ä¿®æ”¹å‘½ä»¤è¡Œå‚æ•°ï¼Œå¼ºåˆ¶ä½¿ç”¨AdamW
    sys.argv = [
        'base_train.py',
        '--run=single_npu_fineweb_no_muon',
        '--depth=6',                    # ä¸­ç­‰æ·±åº¦
        '--device_batch_size=4',        # é€‚ä¸­batch size
        '--total_batch_size=8192',      # æ€»batch size
        '--num_iterations=1000',        # æµ‹è¯•ç”¨è¿­ä»£æ¬¡æ•°
        '--embedding_lr=0.001',
        '--unembedding_lr=0.0001',
        '--matrix_lr=0.0005',
        '--grad_clip=1.0',
        '--eval_every=100',
        '--sample_every=500',
        '--core_metric_every=999999',
        '--optimizer=adamw',            # å¼ºåˆ¶ä½¿ç”¨AdamW
        '--verbose'
    ]
    
    try:
        from scripts.base_train import main
        main()
        print("âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
EOF

# æ‰§è¡Œè®­ç»ƒ
python3 temp_single_npu_train.py

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -f temp_single_npu_train.py

echo ""
echo "ğŸ‰ å•NPU FineWebè®­ç»ƒå®Œæˆï¼"
echo ""
echo "å¦‚æœæˆåŠŸï¼Œå¯ä»¥å°è¯•ï¼š"
echo "  1. å¢åŠ æ·±åº¦: depth=8 æˆ– depth=12"
echo "  2. å¢åŠ batch size: device_batch_size=8"
echo "  3. å¢åŠ è¿­ä»£æ¬¡æ•°: num_iterations=4000"