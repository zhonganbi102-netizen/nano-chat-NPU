#!/usr/bin/env python3
"""
ç®€åŒ–çš„NPUè®­ç»ƒè„šæœ¬ï¼Œä¸“é—¨ä¸ºåä¸ºæ˜‡è…¾NPUä¼˜åŒ–
é¿å…è®¾å¤‡æ£€æµ‹é—®é¢˜ï¼Œç›´æ¥ä½¿ç”¨NPUè®­ç»ƒ
"""

import os
import sys
import time
import torch

# å¼ºåˆ¶æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/mnt/linxid615/bza/nanochat-npu')

def main():
    print("=== ğŸš€ ç®€åŒ–NPUè®­ç»ƒè„šæœ¬ ===")
    
    # 1. æ£€æŸ¥NPUç¯å¢ƒ
    print("1. æ£€æŸ¥NPUç¯å¢ƒ...")
    try:
        import torch_npu
        if not torch_npu.npu.is_available():
            print("âŒ NPUä¸å¯ç”¨")
            return
        
        device_count = torch_npu.npu.device_count()
        print(f"âœ… NPUå¯ç”¨ï¼Œè®¾å¤‡æ•°: {device_count}")
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªNPUè®¾å¤‡
        device = torch.device("npu:0")
        torch_npu.npu.set_device(0)
        
    except ImportError:
        print("âŒ torch_npuæœªå®‰è£…")
        return
    except Exception as e:
        print(f"âŒ NPUåˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # 2. å¯¼å…¥nanochatæ¨¡å—
    print("2. å¯¼å…¥æ¨¡å—...")
    try:
        from nanochat.gpt import GPT, GPTConfig
        from nanochat.tokenizer import get_tokenizer
        from nanochat.dataset import list_parquet_files, parquets_iter_batched
        print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 3. æ£€æŸ¥æ•°æ®
    print("3. æ£€æŸ¥æ•°æ®æ–‡ä»¶...")
    try:
        files = list_parquet_files()
        if len(files) == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶")
            print("è¯·å…ˆè¿è¡Œ: rm -rf /root/.cache/nanochat/base_data && ln -sf /mnt/linxid615/bza/nanochat-npu/base_data /root/.cache/nanochat/base_data")
            return
        print(f"âœ… æ‰¾åˆ° {len(files)} ä¸ªæ•°æ®æ–‡ä»¶")
    except Exception as e:
        print(f"âŒ æ•°æ®æ£€æŸ¥å¤±è´¥: {e}")
        return
    
    # 4. åŠ è½½tokenizer
    print("4. åŠ è½½tokenizer...")
    try:
        tokenizer = get_tokenizer()
        vocab_size = tokenizer.get_vocab_size()
        print(f"âœ… TokenizeråŠ è½½æˆåŠŸ: vocab_size={vocab_size}")
    except Exception as e:
        print(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
        return
    
    # 5. åˆ›å»ºæ¨¡å‹
    print("5. åˆ›å»ºæ¨¡å‹...")
    try:
        # å°æ¨¡å‹é…ç½®
        depth = 8
        max_seq_len = 256
        model_dim = depth * 64
        num_heads = max(1, (model_dim + 127) // 128)
        
        model_config = GPTConfig(
            sequence_len=max_seq_len,
            vocab_size=vocab_size,
            n_layer=depth,
            n_head=num_heads,
            n_kv_head=num_heads,
            n_embd=model_dim
        )
        
        # åœ¨CPUä¸Šåˆ›å»ºæ¨¡å‹ï¼Œç„¶åç§»åŠ¨åˆ°NPU
        model = GPT(model_config)
        model = model.to(device)
        
        num_params = sum(p.numel() for p in model.parameters())
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ: {num_params:,} å‚æ•°")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 6. ç®€åŒ–çš„æ•°æ®åŠ è½½
    print("6. åˆ›å»ºæ•°æ®è¿­ä»£å™¨...")
    try:
        def simple_data_loader():
            """ç®€åŒ–çš„æ•°æ®åŠ è½½å™¨ï¼Œç›´æ¥ä½¿ç”¨NPU"""
            batch_size = 4
            seq_len = 256
            
            for batch in parquets_iter_batched("train", start=0, step=1):
                if len(batch) == 0:
                    continue
                
                # ä½¿ç”¨tokenizerç¼–ç 
                try:
                    tokens_list = tokenizer.encode(batch[:batch_size], prepend=tokenizer.get_bos_token_id())
                    
                    # åˆ›å»ºæ‰¹æ¬¡
                    inputs_list = []
                    targets_list = []
                    
                    for tokens in tokens_list:
                        if len(tokens) > seq_len + 1:
                            tokens = tokens[:seq_len + 1]
                        elif len(tokens) < seq_len + 1:
                            # å¡«å……åˆ°æ‰€éœ€é•¿åº¦
                            tokens = tokens + [tokenizer.get_bos_token_id()] * (seq_len + 1 - len(tokens))
                        
                        inputs = torch.tensor(tokens[:-1], dtype=torch.int32)
                        targets = torch.tensor(tokens[1:], dtype=torch.int64)
                        
                        inputs_list.append(inputs)
                        targets_list.append(targets)
                    
                    if len(inputs_list) == 0:
                        continue
                        
                    # å †å å¹¶ç§»åŠ¨åˆ°NPU
                    inputs_batch = torch.stack(inputs_list).to(device)
                    targets_batch = torch.stack(targets_list).to(device)
                    
                    yield inputs_batch, targets_batch
                    
                except Exception as e:
                    print(f"æ•°æ®å¤„ç†é”™è¯¯: {e}")
                    continue
        
        data_loader = simple_data_loader()
        print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 7. åˆ›å»ºä¼˜åŒ–å™¨
    print("7. åˆ›å»ºä¼˜åŒ–å™¨...")
    try:
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        print("âœ… ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–å™¨åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # 8. è®­ç»ƒå¾ªç¯
    print("8. å¼€å§‹è®­ç»ƒ...")
    try:
        model.train()
        autocast_ctx = torch.amp.autocast(device_type="npu", dtype=torch.bfloat16)
        
        for step, (x, y) in enumerate(data_loader):
            if step >= 10:  # åªè®­ç»ƒ10æ­¥ä½œä¸ºæµ‹è¯•
                break
                
            print(f"\næ­¥éª¤ {step + 1}/10")
            print(f"  è¾“å…¥å½¢çŠ¶: {x.shape}, è®¾å¤‡: {x.device}")
            print(f"  ç›®æ ‡å½¢çŠ¶: {y.shape}, è®¾å¤‡: {y.device}")
            
            # å‰å‘ä¼ æ’­
            torch_npu.npu.synchronize()
            t0 = time.time()
            
            with autocast_ctx:
                loss = model(x, y)
            
            print(f"  æŸå¤±: {loss.item():.4f}")
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            torch_npu.npu.synchronize()
            t1 = time.time()
            
            dt = t1 - t0
            tokens_per_sec = (x.numel()) / dt
            memory_mb = torch_npu.npu.memory_allocated(0) / 1024**2
            
            print(f"  æ—¶é—´: {dt*1000:.1f}ms")
            print(f"  é€Ÿåº¦: {tokens_per_sec:,.0f} tokens/sec")
            print(f"  å†…å­˜: {memory_mb:.1f}MB")
        
        print("\nğŸ‰ è®­ç»ƒæµ‹è¯•å®Œæˆï¼")
        print("NPUè®­ç»ƒç¯å¢ƒå·¥ä½œæ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒäº†")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

if __name__ == "__main__":
    main()