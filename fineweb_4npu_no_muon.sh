#!/bin/bash

# 4NPU FineWebè®­ç»ƒè„šæœ¬ - å®Œå…¨æ— Muonç‰ˆæœ¬
# 4NPU FineWeb training script - completely Muon-free version

set -e

echo "ðŸš€ 4NPU FineWebè®­ç»ƒ - æ— Muonç‰ˆæœ¬"
echo ""

# 1. çŽ¯å¢ƒè®¾ç½®
echo "1. è®¾ç½®4NPUçŽ¯å¢ƒ..."
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# 4NPUåˆ†å¸ƒå¼çŽ¯å¢ƒ
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29900
export TORCH_COMPILE_DISABLE=1
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:128
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1
export NPU_CALCULATE_DEVICE=0,1,2,3
export OMP_NUM_THREADS=1

echo "âœ… 4NPUçŽ¯å¢ƒé…ç½®å®Œæˆ"

# 2. å¼ºåŠ›æ¸…ç†
echo ""
echo "2. å¼ºåŠ›æ¸…ç†NPUçŽ¯å¢ƒ..."
if [ -f "./emergency_npu_cleanup.sh" ]; then
    ./emergency_npu_cleanup.sh
    sleep 5
fi

# 3. åˆ›å»º4NPUæ— Muonä¼˜åŒ–å™¨è¡¥ä¸
echo ""
echo "3. åˆ›å»º4NPUæ— Muonä¼˜åŒ–å™¨è¡¥ä¸..."
cat > fineweb_4npu_no_muon_patch.py << 'EOF'
import torch
from nanochat.gpt import GPT

def fineweb_4npu_adamw_optimizers(self, unembedding_lr=0.001, embedding_lr=0.01, matrix_lr=0.01, weight_decay=0.0):
    """
    4NPU FineWebä¸“ç”¨AdamWä¼˜åŒ–å™¨ - å®Œå…¨æ›¿ä»£Muon
    é’ˆå¯¹å¤§è§„æ¨¡è®­ç»ƒä¼˜åŒ–
    """
    print("ðŸ”§ 4NPU FineWeb: ä½¿ç”¨çº¯AdamWä¼˜åŒ–å™¨ï¼ˆæ— Muonï¼Œåˆ†å¸ƒå¼å‹å¥½ï¼‰")
    
    # è¯¦ç»†å‚æ•°åˆ†ç»„
    embedding_params = []
    unembedding_params = []
    attention_params = []
    ffn_params = []
    layernorm_params = []
    other_params = []
    
    for name, param in self.named_parameters():
        if 'emb_tok' in name:
            embedding_params.append(param)
        elif 'unembed' in name:
            unembedding_params.append(param)
        elif 'attn' in name:
            attention_params.append(param)
        elif 'ffn' in name or 'mlp' in name:
            ffn_params.append(param)
        elif 'norm' in name or 'ln' in name:
            layernorm_params.append(param)
        else:
            other_params.append(param)
    
    print(f"å‚æ•°åˆ†ç»„ç»Ÿè®¡:")
    print(f"  Embedding: {len(embedding_params)} å‚æ•°")
    print(f"  Unembedding: {len(unembedding_params)} å‚æ•°")
    print(f"  Attention: {len(attention_params)} å‚æ•°")
    print(f"  FFN: {len(ffn_params)} å‚æ•°")
    print(f"  LayerNorm: {len(layernorm_params)} å‚æ•°")
    print(f"  Other: {len(other_params)} å‚æ•°")
    
    optimizers = []
    
    # Embeddingä¼˜åŒ–å™¨ï¼ˆé«˜å­¦ä¹ çŽ‡ï¼‰
    if embedding_params:
        emb_opt = torch.optim.AdamW(
            embedding_params,
            lr=embedding_lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,
            fused=False
        )
        optimizers.append(emb_opt)
        print(f"  âœ… Embedding AdamW: lr={embedding_lr}")
    
    # Unembeddingä¼˜åŒ–å™¨ï¼ˆä½Žå­¦ä¹ çŽ‡ï¼‰
    if unembedding_params:
        unemb_opt = torch.optim.AdamW(
            unembedding_params,
            lr=unembedding_lr,
            weight_decay=0.0,
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,
            fused=False
        )
        optimizers.append(unemb_opt)
        print(f"  âœ… Unembedding AdamW: lr={unembedding_lr}")
    
    # Attentionå’ŒFFNå‚æ•°ï¼ˆä¸­ç­‰å­¦ä¹ çŽ‡ï¼‰
    matrix_params = attention_params + ffn_params + other_params
    if matrix_params:
        matrix_opt = torch.optim.AdamW(
            matrix_params,
            lr=matrix_lr,
            weight_decay=0.0,
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,
            fused=False
        )
        optimizers.append(matrix_opt)
        print(f"  âœ… Matrix AdamW: lr={matrix_lr}")
    
    # LayerNormå‚æ•°ï¼ˆä½Žå­¦ä¹ çŽ‡ï¼Œæ— weight decayï¼‰
    if layernorm_params:
        ln_opt = torch.optim.AdamW(
            layernorm_params,
            lr=matrix_lr * 0.5,
            weight_decay=0.0,
            betas=(0.9, 0.95),
            eps=1e-8,
            foreach=False,
            fused=False
        )
        optimizers.append(ln_opt)
        print(f"  âœ… LayerNorm AdamW: lr={matrix_lr * 0.5}")
    
    print(f"ðŸŽ¯ æ€»å…±åˆ›å»ºäº† {len(optimizers)} ä¸ªåˆ†å¸ƒå¼å‹å¥½çš„AdamWä¼˜åŒ–å™¨")
    return optimizers

# æ›¿æ¢åŽŸå§‹æ–¹æ³•
GPT.setup_optimizers = fineweb_4npu_adamw_optimizers
print("âœ… 4NPU FineWebæ— Muonä¼˜åŒ–å™¨è¡¥ä¸å·²åº”ç”¨")
EOF

# 4. æ£€æŸ¥tokenizer
echo ""
echo "4. æ£€æŸ¥tokenizer..."
if [ ! -f "tokenizer/tokenizer.json" ] && [ ! -f ~/.cache/nanochat/tokenizer/tokenizer.pkl ]; then
    echo "è®­ç»ƒtokenizer..."
    python -m scripts.tok_train
else
    echo "âœ… tokenizerå·²å­˜åœ¨"
fi

# 5. å¼€å§‹4NPU FineWebè®­ç»ƒ
echo ""
echo "5. å¼€å§‹4NPU FineWebè®­ç»ƒï¼ˆæ— Muonï¼‰..."
echo ""
echo "é…ç½®:"
echo "  - æ¨¡åž‹æ·±åº¦: 10å±‚"
echo "  - æ‰¹æ¬¡å¤§å°: æ¯è®¾å¤‡6, æ€»98304"
echo "  - è®­ç»ƒæ­¥æ•°: 3000æ­¥"
echo "  - ä¼˜åŒ–å™¨: çº¯AdamWï¼ˆåˆ†å¸ƒå¼å‹å¥½ï¼‰"
echo "  - é¢„è®¡æ—¶é—´: 2-3å°æ—¶"
echo ""

python3 -c "import fineweb_4npu_no_muon_patch" && \
PYTHONPATH=. torchrun --nproc_per_node=4 \
    --master_addr=127.0.0.1 \
    --master_port=29900 \
    scripts/base_train.py \
    --model_tag=fineweb_4npu_no_muon_d10 \
    --depth=10 \
    --device_batch_size=6 \
    --total_batch_size=98304 \
    --num_iterations=3000 \
    --embedding_lr=0.02 \
    --unembedding_lr=0.001 \
    --matrix_lr=0.008 \
    --grad_clip=0.8 \
    --eval_every=300 \
    --sample_every=900 \
    --core_metric_every=999999 \
    --verbose

# 6. æ¸…ç†
rm -f fineweb_4npu_no_muon_patch.py

echo ""
echo "ðŸŽ‰ 4NPU FineWebæ— Muonè®­ç»ƒå®Œæˆï¼"
echo "æ¨¡åž‹ä¿å­˜: ~/.cache/nanochat/base_checkpoints/fineweb_4npu_no_muon_d10/"