#!/bin/bash

echo "ğŸš€ å¿«é€Ÿå¯åŠ¨4NPUè®­ç»ƒ..."

# è®¾ç½®ç¯å¢ƒå˜é‡
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1

# æ£€æŸ¥NPUçŠ¶æ€
echo "æ£€æŸ¥NPUçŠ¶æ€..."
python3 -c "
import torch_npu
print(f'å¯ç”¨NPUæ•°é‡: {torch_npu.npu.device_count()}')
for i in range(min(4, torch_npu.npu.device_count())):
    print(f'NPU {i}: {torch_npu.npu.get_device_name(i)}')
"

# æ¸…ç†ä¹‹å‰çš„è¿›ç¨‹
echo "æ¸…ç†ä¹‹å‰çš„è®­ç»ƒè¿›ç¨‹..."
pkill -f "python.*base_train.py" 2>/dev/null || true
pkill -f "torchrun" 2>/dev/null || true
sleep 2

# å¯åŠ¨4NPUè®­ç»ƒ
echo "å¯åŠ¨4NPUåˆ†å¸ƒå¼è®­ç»ƒ..."
echo "é…ç½®: 4ä¸ªNPU, depth=12, device_batch_size=8, total_batch_size=262144"
echo ""

# ç”Ÿæˆè¿è¡Œåç§°
RUN_NAME="4npu_quick_$(date +%Y%m%d_%H%M%S)"
echo "è¿è¡Œåç§°: $RUN_NAME"

torchrun --standalone --nproc_per_node=4 scripts/base_train.py \
    --depth 12 \
    --device_batch_size 8 \
    --total_batch_size 262144 \
    --run "$RUN_NAME"

echo ""
echo "è®­ç»ƒå®Œæˆ: $(date)"