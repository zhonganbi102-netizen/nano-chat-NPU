#!/bin/bash

echo "=== ğŸ¯ åˆç†æ•°æ®é‡ä¸‹è½½è„šæœ¬ (20GBç‰ˆæœ¬) ==="

# è®¾ç½®å‚æ•°
export HF_ENDPOINT=https://hf-mirror.com
BASE_DATA_DIR="./base_data"
TARGET_SIZE_GB=20
APPROX_FILE_SIZE_MB=90  # æ¯ä¸ªæ–‡ä»¶çº¦90MB
TARGET_FILES=$((TARGET_SIZE_GB * 1024 / APPROX_FILE_SIZE_MB))  # çº¦222ä¸ªæ–‡ä»¶

echo "ç›®æ ‡å¤§å°: ${TARGET_SIZE_GB}GB"
echo "é¢„è®¡éœ€è¦æ–‡ä»¶æ•°: $TARGET_FILES"
echo "è¿™è¶³å¤Ÿè®­ç»ƒä¸€ä¸ªé«˜è´¨é‡çš„å°åˆ°ä¸­å‹æ¨¡å‹ï¼"

mkdir -p "$BASE_DATA_DIR"

# æ£€æŸ¥å·²ä¸‹è½½æ–‡ä»¶
existing_files=$(ls "$BASE_DATA_DIR"/*.parquet 2>/dev/null | wc -l)
echo "å·²ä¸‹è½½æ–‡ä»¶: $existing_files"

if [ $existing_files -ge $TARGET_FILES ]; then
    echo "âœ… å·²æœ‰è¶³å¤Ÿæ•°æ®æ–‡ä»¶ï¼Œæ— éœ€ä¸‹è½½æ›´å¤š"
    echo "å½“å‰æ•°æ®é‡: $(du -sh $BASE_DATA_DIR | cut -f1)"
    exit 0
fi

echo ""
echo "å¼€å§‹ä¸‹è½½ $TARGET_FILES ä¸ªæ–‡ä»¶ (çº¦${TARGET_SIZE_GB}GB)..."

# ä¸‹è½½ç­–ç•¥é€‰æ‹©
echo "ä¸‹è½½æ¨¡å¼:"
echo "1) è¿ç»­ä¸‹è½½ (0-$TARGET_FILES)"
echo "2) åˆ†æ•£ä¸‹è½½ (æ›´å¥½çš„æ•°æ®å¤šæ ·æ€§)"
echo "3) å¿«é€Ÿä¸‹è½½ (å¹¶è¡Œ)"

read -p "é€‰æ‹©æ¨¡å¼ (1-3): " mode

case $mode in
    1)
        echo "=== è¿ç»­ä¸‹è½½æ¨¡å¼ ==="
        for i in $(seq $existing_files $((TARGET_FILES-1))); do
            filename=$(printf "shard_%05d.parquet" $i)
            echo "ä¸‹è½½ $filename ($((i+1))/$TARGET_FILES)..."
            hf download --repo-type dataset karpathy/fineweb-edu-100b-shuffle "$filename" --local-dir "$BASE_DATA_DIR"
            
            # æ¯ä¸‹è½½10ä¸ªæ–‡ä»¶æ˜¾ç¤ºè¿›åº¦
            if [ $((i % 10)) -eq 0 ]; then
                current_size=$(du -sh "$BASE_DATA_DIR" 2>/dev/null | cut -f1)
                echo "ğŸ“Š è¿›åº¦: $((i+1))/$TARGET_FILES, å½“å‰å¤§å°: $current_size"
            fi
        done
        ;;
    2)
        echo "=== åˆ†æ•£ä¸‹è½½æ¨¡å¼ ==="
        # åˆ†æ•£åœ¨æ•´ä¸ªæ•°æ®é›†ä¸­é€‰æ‹©æ–‡ä»¶ï¼Œè·å¾—æ›´å¥½çš„å¤šæ ·æ€§
        step=$((1823 / TARGET_FILES))
        for i in $(seq 0 $((TARGET_FILES-1))); do
            file_index=$((i * step))
            filename=$(printf "shard_%05d.parquet" $file_index)
            
            if [ ! -f "$BASE_DATA_DIR/$filename" ]; then
                echo "ä¸‹è½½ $filename ($((i+1))/$TARGET_FILES)..."
                hf download --repo-type dataset karpathy/fineweb-edu-100b-shuffle "$filename" --local-dir "$BASE_DATA_DIR"
            fi
        done
        ;;
    3)
        echo "=== å¿«é€Ÿå¹¶è¡Œä¸‹è½½æ¨¡å¼ ==="
        seq $existing_files $((TARGET_FILES-1)) | xargs -n 1 -P 3 -I {} bash -c '
            filename=$(printf "shard_%05d.parquet" {})
            if [ ! -f "'$BASE_DATA_DIR'/$filename" ]; then
                echo "ä¸‹è½½ $filename..."
                hf download --repo-type dataset karpathy/fineweb-edu-100b-shuffle "$filename" --local-dir "'$BASE_DATA_DIR'"
            fi
        '
        ;;
esac

# æœ€ç»ˆç»Ÿè®¡
echo ""
echo "=== ğŸ“Š ä¸‹è½½å®Œæˆç»Ÿè®¡ ==="
final_files=$(ls "$BASE_DATA_DIR"/*.parquet 2>/dev/null | wc -l)
total_size=$(du -sh "$BASE_DATA_DIR" 2>/dev/null | cut -f1)

echo "æ–‡ä»¶æ•°é‡: $final_files"
echo "æ€»å¤§å°: $total_size"

# éªŒè¯æ•°æ®
python3 -c "
import pandas as pd
import os

base_data_dir = '$BASE_DATA_DIR'
files = sorted([f for f in os.listdir(base_data_dir) if f.endswith('.parquet')])

total_rows = 0
for i, filename in enumerate(files[:5]):  # æ£€æŸ¥å‰5ä¸ªæ–‡ä»¶
    filepath = os.path.join(base_data_dir, filename)
    try:
        df = pd.read_parquet(filepath)
        rows = len(df)
        total_rows += rows
        if i == 0:
            print(f'ç¤ºä¾‹æ–‡æœ¬: {df[\"text\"].iloc[0][:100]}...')
    except Exception as e:
        print(f'âŒ {filename} éªŒè¯å¤±è´¥: {e}')

estimated_total_rows = total_rows * len(files) // min(5, len(files))
print(f'\\nâœ… ä¼°è®¡æ€»è¡Œæ•°: {estimated_total_rows:,}')
print(f'ä¼°è®¡æ€»tokens: {estimated_total_rows * 512:,} (å‡è®¾å¹³å‡512 tokens/è¡Œ)')
print('\\nğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼')
"

echo ""
echo "=== ğŸš€ æ¨èè®­ç»ƒå‘½ä»¤ ==="
echo "# å•NPUåŸºç¡€è®­ç»ƒ (å°æ¨¡å‹)"
echo "python scripts/base_train.py --depth=8 --device_batch_size=16"
echo ""
echo "# å¤šNPUè®­ç»ƒ (ä¸­å‹æ¨¡å‹)"  
echo "torchrun --standalone --nproc_per_node=8 scripts/base_train.py \\"
echo "    --depth=12 --device_batch_size=16 --total_batch_size=262144"
echo ""
echo "è¿™ä¸ªæ•°æ®é‡è¶³å¤Ÿè®­ç»ƒå‡ºé«˜è´¨é‡çš„æ¨¡å‹ï¼Œè€Œä¸”è®­ç»ƒæ—¶é—´åˆç†ï¼"