#!/bin/bash

# ä¿®å¤æƒé™å¹¶è¿è¡ŒFineWebè®­ç»ƒ
# Fix permissions and run FineWeb training

set -e

echo "=== ä¿®å¤æƒé™å¹¶å¯åŠ¨FineWebè®­ç»ƒ ==="

# 1. ä¿®å¤æ‰€æœ‰è„šæœ¬çš„æ‰§è¡Œæƒé™
echo "1. ä¿®å¤æ‰§è¡Œæƒé™..."
chmod +x *.sh 2>/dev/null || true
chmod +x *.py 2>/dev/null || true

# 2. å¼ºåˆ¶æ¸…ç†NPUç¯å¢ƒ
echo "2. å¼ºåˆ¶æ¸…ç†NPUç¯å¢ƒ..."
pkill -f python || echo "æ²¡æœ‰Pythonè¿›ç¨‹"
sleep 3

# æ¸…ç†NPUç¼“å­˜
python3 -c "
import torch
try:
    import torch_npu
    import gc
    if torch_npu.npu.is_available():
        for i in range(torch_npu.npu.device_count()):
            torch_npu.npu.set_device(i)
            torch_npu.npu.empty_cache()
        gc.collect()
        print('âœ… NPUç¼“å­˜å·²æ¸…ç†')
except Exception as e:
    print(f'æ¸…ç†å¤±è´¥: {e}')
" || echo "NPUæ¸…ç†å¤±è´¥ï¼Œç»§ç»­..."

# 3. è®¾ç½®ä¼˜åŒ–çš„ç¯å¢ƒå˜é‡
export ASCEND_RT_VISIBLE_DEVICES=0
export WORLD_SIZE=1
export RANK=0
export LOCAL_RANK=0
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500

# NPUä¼˜åŒ–è®¾ç½®
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:128
export NPU_COMPILE_DISABLE=1
export TORCH_COMPILE_DISABLE=1
export TORCHDYNAMO_DISABLE=1

echo "ç¯å¢ƒé…ç½®:"
echo "  PYTORCH_NPU_ALLOC_CONF: $PYTORCH_NPU_ALLOC_CONF"
echo "  ç¼–è¯‘ä¼˜åŒ–: ç¦ç”¨"

# 4. æ£€æŸ¥æ•°æ®æ–‡ä»¶
echo "3. æ£€æŸ¥FineWebæ•°æ®..."
if [ -d "~/.cache/nanochat/tokenized_data" ]; then
    data_count=$(find ~/.cache/nanochat/tokenized_data -name "*.parquet" | wc -l)
    echo "âœ… å‘ç° $data_count ä¸ªæ•°æ®æ–‡ä»¶"
else
    echo "âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œéœ€è¦å…ˆä¸‹è½½æ•°æ®"
    exit 1
fi

# 5. è®­ç»ƒtokenizerï¼ˆå¦‚æœéœ€è¦ï¼‰
echo "4. æ£€æŸ¥tokenizer..."
if [ ! -f "~/.cache/nanochat/tokenizer/tokenizer.pkl" ]; then
    echo "è®­ç»ƒtokenizer..."
    python -m scripts.tok_train || echo "tokenizerè®­ç»ƒå¤±è´¥ï¼Œç»§ç»­..."
else
    echo "âœ… tokenizerå·²å­˜åœ¨"
fi

# 6. å¯åŠ¨NPUè®­ç»ƒ
echo "5. å¯åŠ¨FineWeb NPUè®­ç»ƒ..."
echo "é…ç½®: depth=8, batch_size=16, total_batch_size=32768"

python -c "
import os
import sys
import torch
import torch_npu

# ç¦ç”¨ç¼–è¯‘
torch._dynamo.config.disable = True
torch.set_default_device('npu:0')

# æ·»åŠ è·¯å¾„
sys.path.append('.')

# è®¾ç½®è®­ç»ƒå‚æ•° - é€‚åˆFineWebå¤§è§„æ¨¡è®­ç»ƒ
sys.argv = [
    'base_train.py',
    '--run=fineweb_npu_training',
    '--depth=8',
    '--device_batch_size=16',
    '--total_batch_size=32768',
    '--num_iterations=10000'  # å¤§è§„æ¨¡è®­ç»ƒ
]

# è¿è¡Œè®­ç»ƒ
from scripts import base_train
print('ğŸš€ å¼€å§‹FineWebå¤§è§„æ¨¡NPUè®­ç»ƒ...')
base_train.main()
"

echo "âœ… FineWebè®­ç»ƒå®Œæˆï¼"