#!/bin/bash

# FineWebè®­ç»ƒå¯åŠ¨è„šæœ¬ - åŸºäºŽæˆåŠŸçš„4NPUé…ç½®
# ç»“åˆä¹‹å‰æˆåŠŸç»éªŒçš„ç²¾ç¡®è®­ç»ƒå‘½ä»¤

set -e

echo "=== FineWebå¤§è§„æ¨¡è®­ç»ƒå¯åŠ¨ ==="

# 1. çŽ¯å¢ƒå‡†å¤‡
echo "1. çŽ¯å¢ƒå‡†å¤‡..."
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# æ£€æŸ¥æ•°æ®
data_files=$(ls base_data/shard_*.parquet 2>/dev/null | wc -l || echo "0")
if [ "$data_files" -lt 50 ]; then
    echo "âŒ æ•°æ®æ–‡ä»¶ä¸è¶³($data_filesä¸ª)ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®"
    exit 1
fi
echo "âœ… æ£€æµ‹åˆ° $data_files ä¸ªæ•°æ®æ–‡ä»¶"

# 2. æ¸…ç†çŽ¯å¢ƒ
echo "2. æ¸…ç†çŽ¯å¢ƒ..."
if [ -f "./emergency_npu_cleanup.sh" ]; then
    ./emergency_npu_cleanup.sh
    sleep 5
fi

# 3. è®¾ç½®4NPUçŽ¯å¢ƒå˜é‡ (åŸºäºŽæˆåŠŸé…ç½®)
echo "3. è®¾ç½®4NPUçŽ¯å¢ƒå˜é‡..."
export WORLD_SIZE=4
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29520
export TORCH_COMPILE_DISABLE=1
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:64
export HCCL_WHITELIST_DISABLE=1
export HCCL_IF_IP=127.0.0.1

# 4. åˆ›å»ºæˆåŠŸéªŒè¯è¿‡çš„ä¼˜åŒ–å™¨è¡¥ä¸
echo "4. åˆ›å»ºä¼˜åŒ–å™¨è¡¥ä¸..."
cat > temp_fineweb_train_patch.py << EOF
import torch
from nanochat.gpt import GPT

def fineweb_train_optimizers(self, unembedding_lr=0.001, embedding_lr=0.01, matrix_lr=0.01, weight_decay=0.0):
    print("FineWebè®­ç»ƒä¼˜åŒ–å™¨è®¾ç½®: å…¨éƒ¨AdamW (åŸºäºŽæˆåŠŸé…ç½®)")
    embedding_params = [p for n, p in self.named_parameters() if 'emb_tok' in n]
    other_params = [p for n, p in self.named_parameters() if 'emb_tok' not in n]
    opts = []
    if embedding_params:
        opts.append(torch.optim.AdamW([{'params': embedding_params, 'lr': embedding_lr*0.8, 'initial_lr': embedding_lr*0.8}], lr=embedding_lr*0.8, weight_decay=weight_decay, betas=(0.9, 0.95)))
    if other_params:
        opts.append(torch.optim.AdamW([{'params': other_params, 'lr': matrix_lr*0.8, 'initial_lr': matrix_lr*0.8}], lr=matrix_lr*0.8, weight_decay=0.0, betas=(0.9, 0.95)))
    return opts

GPT.setup_optimizers = fineweb_train_optimizers
print("âœ… FineWebè®­ç»ƒä¼˜åŒ–å™¨è¡¥ä¸å·²åº”ç”¨")
EOF

# 5. è®­ç»ƒtokenizer (å¦‚æžœéœ€è¦)
echo "5. è®­ç»ƒtokenizer..."
if [ ! -f ~/.cache/nanochat/tokenizer/tokenizer.pkl ]; then
    echo "è®­ç»ƒtokenizer..."
    python -m scripts.tok_train
else
    echo "tokenizerå·²å­˜åœ¨ï¼Œè·³è¿‡è®­ç»ƒ"
fi

# 6. å¼€å§‹å¤§è§„æ¨¡baseè®­ç»ƒ
echo "6. å¼€å§‹FineWebå¤§è§„æ¨¡baseè®­ç»ƒ..."
echo ""
echo "è®­ç»ƒé…ç½®:"
echo "  - æ•°æ®æ–‡ä»¶: $data_files ä¸ª"
echo "  - æ¨¡åž‹æ·±åº¦: 12å±‚"
echo "  - 4NPUåˆ†å¸ƒå¼è®­ç»ƒ"
echo "  - æ‰¹æ¬¡å¤§å°: æ¯è®¾å¤‡16, æ€»å…±262144"
echo "  - è®­ç»ƒæ­¥æ•°: 5000"
echo "  - é¢„è®¡æ—¶é—´: 2-4å°æ—¶"
echo ""

python -c "import temp_fineweb_train_patch" && \
torchrun --nproc_per_node=4 \
    --master_addr=127.0.0.1 \
    --master_port=29520 \
    scripts/base_train.py \
    --model_tag=fineweb_base_d12 \
    --depth=12 \
    --device_batch_size=16 \
    --total_batch_size=262144 \
    --num_iterations=5000 \
    --embedding_lr=0.2 \
    --unembedding_lr=0.004 \
    --matrix_lr=0.02 \
    --grad_clip=1.0 \
    --eval_every=250 \
    --sample_every=1000 \
    --core_metric_every=999999

# 7. æ¸…ç†
rm -f temp_fineweb_train_patch.py

echo ""
echo "ðŸŽ‰ FineWebå¤§è§„æ¨¡è®­ç»ƒå®Œæˆï¼"
echo ""
echo "æ¨¡åž‹ä¿å­˜ä½ç½®: ~/.cache/nanochat/base_checkpoints/fineweb_base_d12/"
echo ""
echo "ä¸‹ä¸€æ­¥:"
echo "  - æµ‹è¯•æ¨¡åž‹: python -m scripts.chat_cli"
echo "  - å¯åŠ¨Web: python -m scripts.chat_web"
